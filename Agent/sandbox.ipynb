{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b51feb1",
   "metadata": {},
   "source": [
    "# Generic File Ingestion RAG Pipeline\n",
    "\n",
    "This notebook contains a comprehensive RAG pipeline for analyzing any data file and finding the top 5 best database tables for data ingestion using semantic search and LLM analysis.\n",
    "\n",
    "## Features:\n",
    "- **Multi-format support**: CSV, Excel, JSON, TSV, TXT files\n",
    "- **Automatic domain detection** from column names\n",
    "- **Context-aware semantic query generation**\n",
    "- **SQL agent optimized output format**\n",
    "- **Confidence scoring for automation decisions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce52fd78",
   "metadata": {},
   "source": [
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5f579985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dependencies loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "from uuid import uuid4\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Vector store and embeddings\n",
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_client.models import PointStruct, PayloadSchemaType\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(r'C:\\Users\\axel.grille\\Documents\\rules-engine-agent\\Agent\\.env')\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "print(\"âœ… Dependencies loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2735e096",
   "metadata": {},
   "source": [
    "## Initialize Vector Store and Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3c06da1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Clients initialized\n",
      "ğŸ“Š Available collections: ['maxo_vector_store_v2', 'maxo_vector_store']\n"
     ]
    }
   ],
   "source": [
    "# Initialize OpenAI and Qdrant clients\n",
    "client = OpenAI()\n",
    "encoder = OpenAIEmbeddings()\n",
    "\n",
    "# Initialize Qdrant client\n",
    "qdrant_client = QdrantClient(\n",
    "    url=\"https://456cac0f-558d-40b4-ab89-e103423d7d7e.eu-central-1-0.aws.cloud.qdrant.io:6333\", \n",
    "    api_key=\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIiwiZXhwIjoxODE3MzkwMzg0fQ.Hi3c7w5CVjn_Xdpvh4Z9HEEYPbQXXPznxuXKyEEVTLg\",\n",
    ")\n",
    "\n",
    "print(\"âœ… Clients initialized\")\n",
    "print(f\"ğŸ“Š Available collections: {[col.name for col in qdrant_client.get_collections().collections]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a591a963",
   "metadata": {},
   "source": [
    "## Database Setup (Run Once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9065c682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Login successful\n",
      "âœ… DICO data retrieved\n",
      "âœ… Retrieved 409 tables from database schema\n"
     ]
    }
   ],
   "source": [
    "# DICO API call to get database schema\n",
    "import requests\n",
    "\n",
    "def efficy_api_call():\n",
    "    \"\"\"Fetch database schema from Efficy API\"\"\"\n",
    "    session = requests.Session()\n",
    "    \n",
    "    try:\n",
    "        # Login\n",
    "        login_response = session.post(\n",
    "            \"https://sandbox-5.efficytest.cloud/crm/logon\",\n",
    "            headers={\n",
    "                'X-Efficy-Customer': 'SANDBOX05',\n",
    "                'X-Requested-By': 'User',\n",
    "                'X-Requested-With': 'XMLHttpRequest',\n",
    "                'Content-Type': 'application/x-www-form-urlencoded'\n",
    "            },\n",
    "            data='user=paul&password=Eff1cyDemo!'\n",
    "        )\n",
    "        \n",
    "        if login_response.status_code == 200:\n",
    "            print(\"âœ… Login successful\")\n",
    "            \n",
    "            # DICO request\n",
    "            dico_response = session.get(\n",
    "                \"https://sandbox-5.efficytest.cloud/crm/system/dico\",\n",
    "                headers={\n",
    "                    'X-Requested-By': 'User',\n",
    "                    'X-Requested-With': 'XMLHttpRequest'\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            if dico_response.status_code == 200:\n",
    "                print(\"âœ… DICO data retrieved\")\n",
    "                return dico_response.json()\n",
    "            else:\n",
    "                print(f\"âŒ DICO request failed: {dico_response.status_code}\")\n",
    "                \n",
    "        else:\n",
    "            print(f\"âŒ Login failed: {login_response.status_code}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Request error: {e}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Execute the API call (comment out if dico_data already exists)\n",
    "dico_data = efficy_api_call()\n",
    "if dico_data:\n",
    "    print(f\"âœ… Retrieved {len(dico_data['data']['tables'])} tables from database schema\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "430cd0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generated 286 table chunks\n",
      "âœ… Using existing collection: maxo_vector_store_v2\n"
     ]
    }
   ],
   "source": [
    "# Create stable ID function\n",
    "def stable_id(*parts, length=32): \n",
    "    base = '|'.join(str(p) for p in parts)\n",
    "    return hashlib.sha256(base.encode()).hexdigest()[:length]\n",
    "\n",
    "# Generate table chunks for vector store\n",
    "from chunk_generator import generate_table_ingestion_chunks\n",
    "\n",
    "if 'table_chunks' not in locals():\n",
    "    table_chunks = generate_table_ingestion_chunks(dico_data)\n",
    "\n",
    "print(f\"âœ… Generated {len(table_chunks)} table chunks\")\n",
    "\n",
    "# Create vector store collection\n",
    "collection_name = \"maxo_vector_store_v2\"\n",
    "\n",
    "existing_collections = [col.name for col in qdrant_client.get_collections().collections]\n",
    "if collection_name not in existing_collections: \n",
    "    qdrant_client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=models.VectorParams(\n",
    "            size=len(encoder.embed_query(\"Hello world\")),\n",
    "            distance=models.Distance.COSINE,\n",
    "        ),\n",
    "    )\n",
    "    print(f\"âœ… Created collection: {collection_name}\")\n",
    "else:\n",
    "    print(f\"âœ… Using existing collection: {collection_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "717f89c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Successfully upserted 286 table chunks\n",
      "ğŸ“Š Collection now contains 286 total points\n",
      "âœ… Payload indexes created\n"
     ]
    }
   ],
   "source": [
    "# Create and upload table points to vector store\n",
    "table_points = []\n",
    "\n",
    "for chunk in table_chunks: \n",
    "    chunk_id = stable_id(\n",
    "        chunk.metadata['chunk_type'],\n",
    "        chunk.metadata['primary_table'],\n",
    "        chunk.metadata['table_code']\n",
    "    )\n",
    "\n",
    "    embedding = encoder.embed_query(chunk.page_content)\n",
    "\n",
    "    point = PointStruct(\n",
    "        id=chunk_id, \n",
    "        vector=embedding,\n",
    "        payload={\n",
    "            'content': chunk.page_content,\n",
    "            'chunk_type': chunk.metadata['chunk_type'],\n",
    "            'primary_table': chunk.metadata['primary_table'],\n",
    "            'table_code': chunk.metadata['table_code'],\n",
    "            'table_kind': chunk.metadata['table_kind'],\n",
    "            'field_count': chunk.metadata['field_count'],\n",
    "            'metadata': chunk.metadata\n",
    "        }\n",
    "    )\n",
    "    table_points.append(point)\n",
    "\n",
    "# Upsert points to Qdrant\n",
    "try:\n",
    "    result = qdrant_client.upsert(\n",
    "        collection_name=collection_name, \n",
    "        points=table_points\n",
    "    )\n",
    "    print(f\"âœ… Successfully upserted {len(table_points)} table chunks\")\n",
    "    \n",
    "    collection_info = qdrant_client.get_collection(collection_name)\n",
    "    print(f\"ğŸ“Š Collection now contains {collection_info.points_count} total points\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error during upsert: {e}\")\n",
    "\n",
    "# Create payload indexes for efficient filtering\n",
    "try:\n",
    "    qdrant_client.create_payload_index(\n",
    "        collection_name=collection_name,\n",
    "        field_name=\"chunk_type\",\n",
    "        field_schema=PayloadSchemaType.KEYWORD\n",
    "    )\n",
    "    qdrant_client.create_payload_index(\n",
    "        collection_name=collection_name,\n",
    "        field_name=\"primary_table\",\n",
    "        field_schema=PayloadSchemaType.KEYWORD\n",
    "    )\n",
    "    print(\"âœ… Payload indexes created\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Index creation (may already exist): {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0088eeb9",
   "metadata": {},
   "source": [
    "## Generic File Ingestion RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8574d35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… GenericFileIngestionRAGPipeline class defined\n"
     ]
    }
   ],
   "source": [
    "class GenericFileIngestionRAGPipeline:\n",
    "    \"\"\"\n",
    "    Generic RAG pipeline for analyzing any data file and finding the top 5 best\n",
    "    database tables for data ingestion using semantic search and LLM analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, qdrant_client, encoder, collection_name=\"maxo_vector_store_v2\"):\n",
    "        self.qdrant_client = qdrant_client\n",
    "        self.encoder = encoder\n",
    "        self.collection_name = collection_name\n",
    "        self.llm_client = OpenAI()\n",
    "        self.supported_formats = ['.csv', '.xlsx', '.xls', '.json', '.txt', '.tsv']\n",
    "    \n",
    "    def analyze_file_structure(self, file_path):\n",
    "        \"\"\"Analyze any supported file structure and content\"\"\"\n",
    "        try:\n",
    "            if not os.path.exists(file_path):\n",
    "                return {'error': f'File not found: {file_path}'}\n",
    "            \n",
    "            file_extension = os.path.splitext(file_path)[1].lower()\n",
    "            file_name = os.path.basename(file_path)\n",
    "            \n",
    "            if file_extension not in self.supported_formats:\n",
    "                return {'error': f'Unsupported file format: {file_extension}'}\n",
    "            \n",
    "            # Handle different file types\n",
    "            if file_extension == '.csv':\n",
    "                return self._analyze_csv(file_path, file_name)\n",
    "            elif file_extension in ['.xlsx', '.xls']:\n",
    "                return self._analyze_excel(file_path, file_name)\n",
    "            elif file_extension == '.json':\n",
    "                return self._analyze_json(file_path, file_name)\n",
    "            elif file_extension in ['.txt', '.tsv']:\n",
    "                return self._analyze_text(file_path, file_name)\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {'error': f'Failed to analyze file: {str(e)}'}\n",
    "    \n",
    "    def _analyze_csv(self, file_path, file_name):\n",
    "        \"\"\"Analyze CSV files\"\"\"\n",
    "        df = pd.read_csv(file_path)\n",
    "        return self._create_file_analysis(df, file_name, 'CSV')\n",
    "    \n",
    "    def _analyze_excel(self, file_path, file_name):\n",
    "        \"\"\"Analyze Excel files\"\"\"\n",
    "        df = pd.read_excel(file_path)\n",
    "        return self._create_file_analysis(df, file_name, 'Excel')\n",
    "    \n",
    "    def _analyze_json(self, file_path, file_name):\n",
    "        \"\"\"Analyze JSON files\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        if isinstance(data, list) and len(data) > 0 and isinstance(data[0], dict):\n",
    "            df = pd.DataFrame(data)\n",
    "            return self._create_file_analysis(df, file_name, 'JSON Array')\n",
    "        elif isinstance(data, dict):\n",
    "            df = pd.DataFrame([data])\n",
    "            return self._create_file_analysis(df, file_name, 'JSON Object')\n",
    "        else:\n",
    "            return {'error': 'JSON structure not suitable for tabular analysis'}\n",
    "    \n",
    "    def _analyze_text(self, file_path, file_name):\n",
    "        \"\"\"Analyze text/TSV files\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            first_line = f.readline()\n",
    "        \n",
    "        delimiter = '\\t' if '\\t' in first_line else ',' if ',' in first_line else ';'\n",
    "        df = pd.read_csv(file_path, delimiter=delimiter)\n",
    "        return self._create_file_analysis(df, file_name, 'Text/TSV')\n",
    "    \n",
    "    def _create_file_analysis(self, df, file_name, file_type):\n",
    "        \"\"\"Create standardized file analysis from DataFrame\"\"\"\n",
    "        file_info = {\n",
    "            'file_name': file_name,\n",
    "            'file_type': file_type,\n",
    "            'total_rows': len(df),\n",
    "            'total_columns': len(df.columns),\n",
    "            'columns': df.columns.tolist(),\n",
    "            'sample_data': df.head(2).to_dict('records') if len(df) > 0 else []\n",
    "        }\n",
    "        \n",
    "        # Analyze column types and content\n",
    "        column_analysis = {}\n",
    "        for col in df.columns:\n",
    "            column_analysis[col] = {\n",
    "                'dtype': str(df[col].dtype),\n",
    "                'non_null_count': df[col].notna().sum(),\n",
    "                'null_count': df[col].isna().sum(),\n",
    "                'unique_values': df[col].nunique(),\n",
    "                'sample_values': df[col].dropna().head(3).tolist()\n",
    "            }\n",
    "        \n",
    "        file_info['column_analysis'] = column_analysis\n",
    "        return file_info\n",
    "    \n",
    "    def _infer_data_domain(self, columns):\n",
    "        \"\"Enhanced data domain inference with comprehensive business entity detection\"\n",
    "        columns_lower = [col.lower() for col in columns]\n",
    "        \n",
    "        # Comprehensive domain detection patterns\n",
    "        domain_patterns = {\n",
    "            # Core CRM entities\n",
    "            'leads': [\n",
    "                'lead', 'prospect', 'lead_status', 'source', 'qualification', 'score', 'conversion',\n",
    "                'lead_id', 'prospect_id', 'qualified', 'unqualified', 'mql', 'sql', 'nurture'\n",
    "            ],\n",
    "            'opportunities': [\n",
    "                'opportunity', 'deal', 'pipeline', 'stage', 'probability', 'close_date', 'forecast',\n",
    "                'opp_id', 'deal_id', 'sales_stage', 'win_probability', 'expected_revenue', 'deal_value'\n",
    "            ],\n",
    "            'contacts': [\n",
    "                'contact', 'person', 'individual', 'first_name', 'last_name', 'title', 'relationship',\n",
    "                'contact_id', 'person_id', 'full_name', 'job_title', 'phone', 'mobile', 'email'\n",
    "            ],\n",
    "            'companies': [\n",
    "                'company', 'organization', 'enterprise', 'business', 'industry', 'sector', 'headquarters',\n",
    "                'company_id', 'org_id', 'business_name', 'company_name', 'industry_type', 'company_size'\n",
    "            ],\n",
    "            'activities': [\n",
    "                'activity', 'action', 'event', 'log', 'history', 'timeline', 'interaction',\n",
    "                'activity_id', 'event_id', 'action_type', 'activity_type', 'interaction_type', 'follow_up'\n",
    "            ],\n",
    "            'meetings': [\n",
    "                'meeting', 'appointment', 'schedule', 'calendar', 'attendee', 'agenda', 'duration',\n",
    "                'meeting_id', 'appointment_id', 'scheduled', 'start_time', 'end_time', 'location'\n",
    "            ],\n",
    "            'campaigns': [\n",
    "                'campaign', 'marketing', 'promotion', 'advertising', 'channel', 'target', 'response',\n",
    "                'campaign_id', 'promo_id', 'marketing_campaign', 'campaign_name', 'campaign_type'\n",
    "            ],\n",
    "            'tickets': [\n",
    "                'ticket', 'issue', 'support', 'incident', 'priority', 'resolution', 'escalation',\n",
    "                'ticket_id', 'issue_id', 'support_ticket', 'incident_id', 'case_id', 'help_desk'\n",
    "            ],\n",
    "            'users': [\n",
    "                'user', 'username', 'login', 'profile', 'role', 'permission', 'access',\n",
    "                'user_id', 'account', 'user_name', 'login_name', 'user_role', 'access_level'\n",
    "            ],\n",
    "            \n",
    "            # Extended business domains\n",
    "            'communication': [\n",
    "                'message', 'email', 'mail', 'subject', 'sender', 'recipient', 'date',\n",
    "                'corps du message', 'mail expÃ©diteur', 'mail destinataire', 'objet', 'visite_mail'\n",
    "            ],\n",
    "            'sales_orders': ['order', 'product', 'price', 'quantity', 'total', 'invoice', 'payment'],\n",
    "            'financial': ['amount', 'cost', 'revenue', 'budget', 'transaction', 'account', 'currency'],\n",
    "            'hr_employee': ['employee', 'staff', 'salary', 'department', 'position', 'hire', 'manager'],\n",
    "            'inventory': ['item', 'stock', 'warehouse', 'supplier', 'category', 'sku', 'unit'],\n",
    "            'project': ['project', 'task', 'milestone', 'deadline', 'status', 'resource', 'team'],\n",
    "            'logistics': ['shipment', 'delivery', 'tracking', 'carrier', 'destination', 'weight']\n",
    "        }\n",
    "        \n",
    "        # Calculate domain scores with weighted importance\n",
    "        domain_scores = {}\n",
    "        for domain, keywords in domain_patterns.items():\n",
    "            score = 0\n",
    "            matched_keywords = []\n",
    "            \n",
    "            for keyword in keywords:\n",
    "                for col in columns_lower:\n",
    "                    if keyword in col:\n",
    "                        # Weight exact matches higher\n",
    "                        if keyword == col:\n",
    "                            score += 3\n",
    "                        # Weight ID fields higher (strong indicators)\n",
    "                        elif keyword.endswith('_id') and keyword in col:\n",
    "                            score += 2.5\n",
    "                        # Regular substring matches\n",
    "                        else:\n",
    "                            score += 1\n",
    "                        matched_keywords.append(keyword)\n",
    "                        break\n",
    "            \n",
    "            domain_scores[domain] = {\n",
    "                'score': score,\n",
    "                'matched_keywords': list(set(matched_keywords))\n",
    "            }\n",
    "        \n",
    "        # Get best matching domain\n",
    "        best_domain = max(domain_scores, key=lambda x: domain_scores[x]['score']) if domain_scores else 'general'\n",
    "        best_score = domain_scores[best_domain]['score'] if best_domain != 'general' else 0\n",
    "        \n",
    "        # Enhanced domain mapping with confidence indicators\n",
    "        domain_mapping = {\n",
    "            'leads': {\n",
    "                'primary_domain': 'lead management and prospecting', \n",
    "                'business_area': 'sales lead generation', \n",
    "                'data_category': 'leads',\n",
    "                'table_hints': ['lead', 'prospect', 'qualification']\n",
    "            },\n",
    "            'opportunities': {\n",
    "                'primary_domain': 'sales opportunity tracking', \n",
    "                'business_area': 'sales pipeline management', \n",
    "                'data_category': 'opportunities',\n",
    "                'table_hints': ['opportunity', 'deal', 'sales_pipeline']\n",
    "            },\n",
    "            'contacts': {\n",
    "                'primary_domain': 'contact and person management', \n",
    "                'business_area': 'relationship management', \n",
    "                'data_category': 'contacts',\n",
    "                'table_hints': ['contact', 'person', 'individual']\n",
    "            },\n",
    "            'companies': {\n",
    "                'primary_domain': 'company and organization management', \n",
    "                'business_area': 'corporate data management', \n",
    "                'data_category': 'companies',\n",
    "                'table_hints': ['company', 'organization', 'enterprise']\n",
    "            },\n",
    "            'activities': {\n",
    "                'primary_domain': 'activity and event tracking', \n",
    "                'business_area': 'interaction management', \n",
    "                'data_category': 'activities',\n",
    "                'table_hints': ['activity', 'event', 'action', 'visit']\n",
    "            },\n",
    "            'meetings': {\n",
    "                'primary_domain': 'meeting and calendar management', \n",
    "                'business_area': 'scheduling and appointments', \n",
    "                'data_category': 'meetings',\n",
    "                'table_hints': ['meeting', 'appointment', 'schedule']\n",
    "            },\n",
    "            'campaigns': {\n",
    "                'primary_domain': 'marketing campaign management', \n",
    "                'business_area': 'marketing operations', \n",
    "                'data_category': 'campaigns',\n",
    "                'table_hints': ['campaign', 'marketing', 'promotion']\n",
    "            },\n",
    "            'tickets': {\n",
    "                'primary_domain': 'ticketing and support management', \n",
    "                'business_area': 'customer support', \n",
    "                'data_category': 'tickets',\n",
    "                'table_hints': ['ticket', 'support', 'incident']\n",
    "            },\n",
    "            'users': {\n",
    "                'primary_domain': 'user and account management', \n",
    "                'business_area': 'system administration', \n",
    "                'data_category': 'users',\n",
    "                'table_hints': ['user', 'account', 'profile']\n",
    "            },\n",
    "            'communication': {\n",
    "                'primary_domain': 'communication and messaging', \n",
    "                'business_area': 'correspondence', \n",
    "                'data_category': 'communication',\n",
    "                'table_hints': ['mail', 'email', 'message', 'visit']\n",
    "            },\n",
    "            'sales_orders': {'primary_domain': 'sales and order management', 'business_area': 'sales operations', 'data_category': 'transactional', 'table_hints': ['order', 'sale', 'invoice']},\n",
    "            'financial': {'primary_domain': 'financial and accounting', 'business_area': 'finance', 'data_category': 'financial', 'table_hints': ['financial', 'accounting', 'budget']},\n",
    "            'hr_employee': {'primary_domain': 'human resources', 'business_area': 'HR management', 'data_category': 'employee', 'table_hints': ['employee', 'staff', 'hr']},\n",
    "            'inventory': {'primary_domain': 'inventory and stock management', 'business_area': 'supply chain', 'data_category': 'inventory', 'table_hints': ['inventory', 'stock', 'product']},\n",
    "            'project': {'primary_domain': 'project management', 'business_area': 'project operations', 'data_category': 'project', 'table_hints': ['project', 'task', 'milestone']},\n",
    "            'logistics': {'primary_domain': 'logistics and shipping', 'business_area': 'operations', 'data_category': 'logistics', 'table_hints': ['shipment', 'delivery', 'logistics']},\n",
    "            'general': {'primary_domain': 'business data', 'business_area': 'general operations', 'data_category': 'business', 'table_hints': ['data', 'general']}\n",
    "        }\n",
    "        \n",
    "        result = domain_mapping.get(best_domain, domain_mapping['general'])\n",
    "        \n",
    "        # Add detection metadata for debugging and confidence assessment\n",
    "        result['detection_confidence'] = min(best_score / 5.0, 1.0)  # Normalize to 0-1\n",
    "        result['matched_keywords'] = domain_scores.get(best_domain, {}).get('matched_keywords', [])\n",
    "        result['all_scores'] = {k: v['score'] for k, v in domain_scores.items() if v['score'] > 0}\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467f4c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and upload table points to vector store\n",
    "table_points = []\n",
    "\n",
    "for chunk in table_chunks: \n",
    "    chunk_id = stable_id(\n",
    "        chunk.metadata['chunk_type'],\n",
    "        chunk.metadata['primary_table'],\n",
    "        chunk.metadata['table_code']\n",
    "    )\n",
    "\n",
    "    embedding = encoder.embed_query(chunk.page_content)\n",
    "\n",
    "    point = PointStruct(\n",
    "        id=chunk_id, \n",
    "        vector=embedding,\n",
    "        payload={\n",
    "            'content': chunk.page_content,\n",
    "            'chunk_type': chunk.metadata['chunk_type'],\n",
    "            'primary_table': chunk.metadata['primary_table'],\n",
    "            'table_code': chunk.metadata['table_code'],\n",
    "            'table_kind': chunk.metadata['table_kind'],\n",
    "            'field_count': chunk.metadata['field_count'],\n",
    "            'metadata': chunk.metadata\n",
    "        }\n",
    "    )\n",
    "    table_points.append(point)\n",
    "\n",
    "# Upsert points to Qdrant\n",
    "try:\n",
    "    result = qdrant_client.upsert(\n",
    "        collection_name=collection_name, \n",
    "        points=table_points\n",
    "    )\n",
    "    print(f\"âœ… Successfully upserted {len(table_points)} table chunks\")\n",
    "    \n",
    "    collection_info = qdrant_client.get_collection(collection_name)\n",
    "    print(f\"ğŸ“Š Collection now contains {collection_info.points_count} total points\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error during upsert: {e}\")\n",
    "\n",
    "# Create payload indexes for efficient filtering\n",
    "try:\n",
    "    qdrant_client.create_payload_index(\n",
    "        collection_name=collection_name,\n",
    "        field_name=\"chunk_type\",\n",
    "        field_schema=PayloadSchemaType.KEYWORD\n",
    "    )\n",
    "    qdrant_client.create_payload_index(\n",
    "        collection_name=collection_name,\n",
    "        field_name=\"primary_table\",\n",
    "        field_schema=PayloadSchemaType.KEYWORD\n",
    "    )\n",
    "    print(\"âœ… Payload indexes created\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Index creation (may already exist): {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced37a24",
   "metadata": {},
   "source": [
    "## Generic File Ingestion RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4274bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenericFileIngestionRAGPipeline:\n",
    "    \"\"\"\n",
    "    Generic RAG pipeline for analyzing any data file and finding the top 5 best\n",
    "    database tables for data ingestion using semantic search and LLM analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, qdrant_client, encoder, collection_name=\"maxo_vector_store_v2\"):\n",
    "        self.qdrant_client = qdrant_client\n",
    "        self.encoder = encoder\n",
    "        self.collection_name = collection_name\n",
    "        self.llm_client = OpenAI()\n",
    "        self.supported_formats = ['.csv', '.xlsx', '.xls', '.json', '.txt', '.tsv']\n",
    "    \n",
    "    def analyze_file_structure(self, file_path):\n",
    "        \"\"\"Analyze any supported file structure and content\"\"\"\n",
    "        try:\n",
    "            if not os.path.exists(file_path):\n",
    "                return {'error': f'File not found: {file_path}'}\n",
    "            \n",
    "            file_extension = os.path.splitext(file_path)[1].lower()\n",
    "            file_name = os.path.basename(file_path)\n",
    "            \n",
    "            if file_extension not in self.supported_formats:\n",
    "                return {'error': f'Unsupported file format: {file_extension}'}\n",
    "            \n",
    "            # Handle different file types\n",
    "            if file_extension == '.csv':\n",
    "                return self._analyze_csv(file_path, file_name)\n",
    "            elif file_extension in ['.xlsx', '.xls']:\n",
    "                return self._analyze_excel(file_path, file_name)\n",
    "            elif file_extension == '.json':\n",
    "                return self._analyze_json(file_path, file_name)\n",
    "            elif file_extension in ['.txt', '.tsv']:\n",
    "                return self._analyze_text(file_path, file_name)\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {'error': f'Failed to analyze file: {str(e)}'}\n",
    "    \n",
    "    def _analyze_csv(self, file_path, file_name):\n",
    "        \"\"\"Analyze CSV files\"\"\"\n",
    "        df = pd.read_csv(file_path)\n",
    "        return self._create_file_analysis(df, file_name, 'CSV')\n",
    "    \n",
    "    def _analyze_excel(self, file_path, file_name):\n",
    "        \"\"\"Analyze Excel files\"\"\"\n",
    "        df = pd.read_excel(file_path)\n",
    "        return self._create_file_analysis(df, file_name, 'Excel')\n",
    "    \n",
    "    def _analyze_json(self, file_path, file_name):\n",
    "        \"\"\"Analyze JSON files\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        if isinstance(data, list) and len(data) > 0 and isinstance(data[0], dict):\n",
    "            df = pd.DataFrame(data)\n",
    "            return self._create_file_analysis(df, file_name, 'JSON Array')\n",
    "        elif isinstance(data, dict):\n",
    "            df = pd.DataFrame([data])\n",
    "            return self._create_file_analysis(df, file_name, 'JSON Object')\n",
    "        else:\n",
    "            return {'error': 'JSON structure not suitable for tabular analysis'}\n",
    "    \n",
    "    def _analyze_text(self, file_path, file_name):\n",
    "        \"\"\"Analyze text/TSV files\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            first_line = f.readline()\n",
    "        \n",
    "        delimiter = '\\t' if '\\t' in first_line else ',' if ',' in first_line else ';'\n",
    "        df = pd.read_csv(file_path, delimiter=delimiter)\n",
    "        return self._create_file_analysis(df, file_name, 'Text/TSV')\n",
    "    \n",
    "    def _create_file_analysis(self, df, file_name, file_type):\n",
    "        \"\"\"Create standardized file analysis from DataFrame\"\"\"\n",
    "        file_info = {\n",
    "            'file_name': file_name,\n",
    "            'file_type': file_type,\n",
    "            'total_rows': len(df),\n",
    "            'total_columns': len(df.columns),\n",
    "            'columns': df.columns.tolist(),\n",
    "            'sample_data': df.head(2).to_dict('records') if len(df) > 0 else []\n",
    "        }\n",
    "        \n",
    "        # Analyze column types and content\n",
    "        column_analysis = {}\n",
    "        for col in df.columns:\n",
    "            column_analysis[col] = {\n",
    "                'dtype': str(df[col].dtype),\n",
    "                'non_null_count': df[col].notna().sum(),\n",
    "                'null_count': df[col].isna().sum(),\n",
    "                'unique_values': df[col].nunique(),\n",
    "                'sample_values': df[col].dropna().head(3).tolist()\n",
    "            }\n",
    "        \n",
    "        file_info['column_analysis'] = column_analysis\n",
    "        return file_info\n",
    "    \n",
    "    def _infer_data_domain(self, columns):\n",
    "        \"\"Enhanced data domain inference with comprehensive business entity detection\"\n",
    "        columns_lower = [col.lower() for col in columns]\n",
    "        \n",
    "        # Comprehensive domain detection patterns\n",
    "        domain_patterns = {\n",
    "            # Core CRM entities\n",
    "            'leads': [\n",
    "                'lead', 'prospect', 'lead_status', 'source', 'qualification', 'score', 'conversion',\n",
    "                'lead_id', 'prospect_id', 'qualified', 'unqualified', 'mql', 'sql', 'nurture'\n",
    "            ],\n",
    "            'opportunities': [\n",
    "                'opportunity', 'deal', 'pipeline', 'stage', 'probability', 'close_date', 'forecast',\n",
    "                'opp_id', 'deal_id', 'sales_stage', 'win_probability', 'expected_revenue', 'deal_value'\n",
    "            ],\n",
    "            'contacts': [\n",
    "                'contact', 'person', 'individual', 'first_name', 'last_name', 'title', 'relationship',\n",
    "                'contact_id', 'person_id', 'full_name', 'job_title', 'phone', 'mobile', 'email'\n",
    "            ],\n",
    "            'companies': [\n",
    "                'company', 'organization', 'enterprise', 'business', 'industry', 'sector', 'headquarters',\n",
    "                'company_id', 'org_id', 'business_name', 'company_name', 'industry_type', 'company_size'\n",
    "            ],\n",
    "            'activities': [\n",
    "                'activity', 'action', 'event', 'log', 'history', 'timeline', 'interaction',\n",
    "                'activity_id', 'event_id', 'action_type', 'activity_type', 'interaction_type', 'follow_up'\n",
    "            ],\n",
    "            'meetings': [\n",
    "                'meeting', 'appointment', 'schedule', 'calendar', 'attendee', 'agenda', 'duration',\n",
    "                'meeting_id', 'appointment_id', 'scheduled', 'start_time', 'end_time', 'location'\n",
    "            ],\n",
    "            'campaigns': [\n",
    "                'campaign', 'marketing', 'promotion', 'advertising', 'channel', 'target', 'response',\n",
    "                'campaign_id', 'promo_id', 'marketing_campaign', 'campaign_name', 'campaign_type'\n",
    "            ],\n",
    "            'tickets': [\n",
    "                'ticket', 'issue', 'support', 'incident', 'priority', 'resolution', 'escalation',\n",
    "                'ticket_id', 'issue_id', 'support_ticket', 'incident_id', 'case_id', 'help_desk'\n",
    "            ],\n",
    "            'users': [\n",
    "                'user', 'username', 'login', 'profile', 'role', 'permission', 'access',\n",
    "                'user_id', 'account', 'user_name', 'login_name', 'user_role', 'access_level'\n",
    "            ],\n",
    "            \n",
    "            # Extended business domains\n",
    "            'communication': [\n",
    "                'message', 'email', 'mail', 'subject', 'sender', 'recipient', 'date',\n",
    "                'corps du message', 'mail expÃ©diteur', 'mail destinataire', 'objet', 'visite_mail'\n",
    "            ],\n",
    "            'sales_orders': ['order', 'product', 'price', 'quantity', 'total', 'invoice', 'payment'],\n",
    "            'financial': ['amount', 'cost', 'revenue', 'budget', 'transaction', 'account', 'currency'],\n",
    "            'hr_employee': ['employee', 'staff', 'salary', 'department', 'position', 'hire', 'manager'],\n",
    "            'inventory': ['item', 'stock', 'warehouse', 'supplier', 'category', 'sku', 'unit'],\n",
    "            'project': ['project', 'task', 'milestone', 'deadline', 'status', 'resource', 'team'],\n",
    "            'logistics': ['shipment', 'delivery', 'tracking', 'carrier', 'destination', 'weight']\n",
    "        }\n",
    "        \n",
    "        # Calculate domain scores with weighted importance\n",
    "        domain_scores = {}\n",
    "        for domain, keywords in domain_patterns.items():\n",
    "            score = 0\n",
    "            matched_keywords = []\n",
    "            \n",
    "            for keyword in keywords:\n",
    "                for col in columns_lower:\n",
    "                    if keyword in col:\n",
    "                        # Weight exact matches higher\n",
    "                        if keyword == col:\n",
    "                            score += 3\n",
    "                        # Weight ID fields higher (strong indicators)\n",
    "                        elif keyword.endswith('_id') and keyword in col:\n",
    "                            score += 2.5\n",
    "                        # Regular substring matches\n",
    "                        else:\n",
    "                            score += 1\n",
    "                        matched_keywords.append(keyword)\n",
    "                        break\n",
    "            \n",
    "            domain_scores[domain] = {\n",
    "                'score': score,\n",
    "                'matched_keywords': list(set(matched_keywords))\n",
    "            }\n",
    "        \n",
    "        # Get best matching domain\n",
    "        best_domain = max(domain_scores, key=lambda x: domain_scores[x]['score']) if domain_scores else 'general'\n",
    "        best_score = domain_scores[best_domain]['score'] if best_domain != 'general' else 0\n",
    "        \n",
    "        # Enhanced domain mapping with confidence indicators\n",
    "        domain_mapping = {\n",
    "            'leads': {\n",
    "                'primary_domain': 'lead management and prospecting', \n",
    "                'business_area': 'sales lead generation', \n",
    "                'data_category': 'leads',\n",
    "                'table_hints': ['lead', 'prospect', 'qualification']\n",
    "            },\n",
    "            'opportunities': {\n",
    "                'primary_domain': 'sales opportunity tracking', \n",
    "                'business_area': 'sales pipeline management', \n",
    "                'data_category': 'opportunities',\n",
    "                'table_hints': ['opportunity', 'deal', 'sales_pipeline']\n",
    "            },\n",
    "            'contacts': {\n",
    "                'primary_domain': 'contact and person management', \n",
    "                'business_area': 'relationship management', \n",
    "                'data_category': 'contacts',\n",
    "                'table_hints': ['contact', 'person', 'individual']\n",
    "            },\n",
    "            'companies': {\n",
    "                'primary_domain': 'company and organization management', \n",
    "                'business_area': 'corporate data management', \n",
    "                'data_category': 'companies',\n",
    "                'table_hints': ['company', 'organization', 'enterprise']\n",
    "            },\n",
    "            'activities': {\n",
    "                'primary_domain': 'activity and event tracking', \n",
    "                'business_area': 'interaction management', \n",
    "                'data_category': 'activities',\n",
    "                'table_hints': ['activity', 'event', 'action', 'visit']\n",
    "            },\n",
    "            'meetings': {\n",
    "                'primary_domain': 'meeting and calendar management', \n",
    "                'business_area': 'scheduling and appointments', \n",
    "                'data_category': 'meetings',\n",
    "                'table_hints': ['meeting', 'appointment', 'schedule']\n",
    "            },\n",
    "            'campaigns': {\n",
    "                'primary_domain': 'marketing campaign management', \n",
    "                'business_area': 'marketing operations', \n",
    "                'data_category': 'campaigns',\n",
    "                'table_hints': ['campaign', 'marketing', 'promotion']\n",
    "            },\n",
    "            'tickets': {\n",
    "                'primary_domain': 'ticketing and support management', \n",
    "                'business_area': 'customer support', \n",
    "                'data_category': 'tickets',\n",
    "                'table_hints': ['ticket', 'support', 'incident']\n",
    "            },\n",
    "            'users': {\n",
    "                'primary_domain': 'user and account management', \n",
    "                'business_area': 'system administration', \n",
    "                'data_category': 'users',\n",
    "                'table_hints': ['user', 'account', 'profile']\n",
    "            },\n",
    "            'communication': {\n",
    "                'primary_domain': 'communication and messaging', \n",
    "                'business_area': 'correspondence', \n",
    "                'data_category': 'communication',\n",
    "                'table_hints': ['mail', 'email', 'message', 'visit']\n",
    "            },\n",
    "            'sales_orders': {'primary_domain': 'sales and order management', 'business_area': 'sales operations', 'data_category': 'transactional', 'table_hints': ['order', 'sale', 'invoice']},\n",
    "            'financial': {'primary_domain': 'financial and accounting', 'business_area': 'finance', 'data_category': 'financial', 'table_hints': ['financial', 'accounting', 'budget']},\n",
    "            'hr_employee': {'primary_domain': 'human resources', 'business_area': 'HR management', 'data_category': 'employee', 'table_hints': ['employee', 'staff', 'hr']},\n",
    "            'inventory': {'primary_domain': 'inventory and stock management', 'business_area': 'supply chain', 'data_category': 'inventory', 'table_hints': ['inventory', 'stock', 'product']},\n",
    "            'project': {'primary_domain': 'project management', 'business_area': 'project operations', 'data_category': 'project', 'table_hints': ['project', 'task', 'milestone']},\n",
    "            'logistics': {'primary_domain': 'logistics and shipping', 'business_area': 'operations', 'data_category': 'logistics', 'table_hints': ['shipment', 'delivery', 'logistics']},\n",
    "            'general': {'primary_domain': 'business data', 'business_area': 'general operations', 'data_category': 'business', 'table_hints': ['data', 'general']}\n",
    "        }\n",
    "        \n",
    "        result = domain_mapping.get(best_domain, domain_mapping['general'])\n",
    "        \n",
    "        # Add detection metadata for debugging and confidence assessment\n",
    "        result['detection_confidence'] = min(best_score / 5.0, 1.0)  # Normalize to 0-1\n",
    "        result['matched_keywords'] = domain_scores.get(best_domain, {}).get('matched_keywords', [])\n",
    "        result['all_scores'] = {k: v['score'] for k, v in domain_scores.items() if v['score'] > 0}\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e1ef35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and upload table points to vector store\n",
    "table_points = []\n",
    "\n",
    "for chunk in table_chunks: \n",
    "    chunk_id = stable_id(\n",
    "        chunk.metadata['chunk_type'],\n",
    "        chunk.metadata['primary_table'],\n",
    "        chunk.metadata['table_code']\n",
    "    )\n",
    "\n",
    "    embedding = encoder.embed_query(chunk.page_content)\n",
    "\n",
    "    point = PointStruct(\n",
    "        id=chunk_id, \n",
    "        vector=embedding,\n",
    "        payload={\n",
    "            'content': chunk.page_content,\n",
    "            'chunk_type': chunk.metadata['chunk_type'],\n",
    "            'primary_table': chunk.metadata['primary_table'],\n",
    "            'table_code': chunk.metadata['table_code'],\n",
    "            'table_kind': chunk.metadata['table_kind'],\n",
    "            'field_count': chunk.metadata['field_count'],\n",
    "            'metadata': chunk.metadata\n",
    "        }\n",
    "    )\n",
    "    table_points.append(point)\n",
    "\n",
    "# Upsert points to Qdrant\n",
    "try:\n",
    "    result = qdrant_client.upsert(\n",
    "        collection_name=collection_name, \n",
    "        points=table_points\n",
    "    )\n",
    "    print(f\"âœ… Successfully upserted {len(table_points)} table chunks\")\n",
    "    \n",
    "    collection_info = qdrant_client.get_collection(collection_name)\n",
    "    print(f\"ğŸ“Š Collection now contains {collection_info.points_count} total points\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error during upsert: {e}\")\n",
    "\n",
    "# Create payload indexes for efficient filtering\n",
    "try:\n",
    "    qdrant_client.create_payload_index(\n",
    "        collection_name=collection_name,\n",
    "        field_name=\"chunk_type\",\n",
    "        field_schema=PayloadSchemaType.KEYWORD\n",
    "    )\n",
    "    qdrant_client.create_payload_index(\n",
    "        collection_name=collection_name,\n",
    "        field_name=\"primary_table\",\n",
    "        field_schema=PayloadSchemaType.KEYWORD\n",
    "    )\n",
    "    print(\"âœ… Payload indexes created\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Index creation (may already exist): {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71706e2c",
   "metadata": {},
   "source": [
    "## Generic File Ingestion RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998fe274",
   "metadata": {},
   "source": [
    "## Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ab311cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GENERIC FILE INGESTION RAG PIPELINE ===\n",
      "ğŸ“ Analyzing: Mails.csv\n",
      "\n",
      "Step 1: Analyzing file structure...\n",
      "âœ“ CSV file: 25 columns, 5 rows\n",
      "âœ“ Detected domain: communication and messaging\n",
      "\n",
      "Step 2: Generating semantic search queries...\n",
      "ğŸ” Enhanced with 5 email-specific queries (confidence: 1.00)\n",
      "âœ“ Generated 11 queries for database search\n",
      "\n",
      "Step 3: Searching for relevant database tables...\n",
      "âœ“ Found 43 unique tables across all queries\n",
      "\n",
      "Step 4: Ranking tables by relevance...\n",
      "âœ“ Ranked 43 tables by composite relevance score\n",
      "\n",
      "================================================================================\n",
      "ğŸ¯ INGESTION ANALYSIS: Mails.csv\n",
      "================================================================================\n",
      "ğŸ“Š File: CSV | 5 rows | 25 columns\n",
      "ğŸ·ï¸  Domain: communication and messaging\n",
      "ğŸ¯ Best Table: Mail\n",
      "ğŸ” Confidence: High\n",
      "âš ï¸  Review Required: No\n",
      "ğŸ¤– SQL Agent Ready: Yes\n",
      "\n",
      "ğŸ“‹ TOP 10 DATABASE TABLES:\n",
      "1. Mail (Entity)\n",
      "   Score: 2.032 | Fields: 31 | Matches: 7\n",
      "2. Contact (Entity)\n",
      "   Score: 1.996 | Fields: 70 | Matches: 7\n",
      "3. Sms (Entity)\n",
      "   Score: 1.815 | Fields: 9 | Matches: 6\n",
      "4. Docu_Camp (Relation)\n",
      "   Score: 1.802 | Fields: 5 | Matches: 6\n",
      "5. Docu_Docu (Relation)\n",
      "   Score: 1.606 | Fields: 6 | Matches: 5\n",
      "6. Kbas_Kbas (Relation)\n",
      "   Score: 1.605 | Fields: 5 | Matches: 5\n",
      "7. Docu_Kbas (Relation)\n",
      "   Score: 1.605 | Fields: 5 | Matches: 5\n",
      "8. Docu_Vatr (Relation)\n",
      "   Score: 1.405 | Fields: 6 | Matches: 4\n",
      "9. Daqu_Cont (Relation)\n",
      "   Score: 1.405 | Fields: 7 | Matches: 4\n",
      "10. Objv_Cont (Relation)\n",
      "   Score: 1.404 | Fields: 5 | Matches: 4\n",
      "\n",
      "âœ… Ready for SQL Agent Processing\n",
      "\n",
      "ğŸ’¾ SQL Agent data exported to: Mails_ingestion_analysis.json\n",
      "âœ… Ready for SQL Agent: Mails_ingestion_analysis.json\n",
      "âœ“ Found 43 unique tables across all queries\n",
      "\n",
      "Step 4: Ranking tables by relevance...\n",
      "âœ“ Ranked 43 tables by composite relevance score\n",
      "\n",
      "================================================================================\n",
      "ğŸ¯ INGESTION ANALYSIS: Mails.csv\n",
      "================================================================================\n",
      "ğŸ“Š File: CSV | 5 rows | 25 columns\n",
      "ğŸ·ï¸  Domain: communication and messaging\n",
      "ğŸ¯ Best Table: Mail\n",
      "ğŸ” Confidence: High\n",
      "âš ï¸  Review Required: No\n",
      "ğŸ¤– SQL Agent Ready: Yes\n",
      "\n",
      "ğŸ“‹ TOP 10 DATABASE TABLES:\n",
      "1. Mail (Entity)\n",
      "   Score: 2.032 | Fields: 31 | Matches: 7\n",
      "2. Contact (Entity)\n",
      "   Score: 1.996 | Fields: 70 | Matches: 7\n",
      "3. Sms (Entity)\n",
      "   Score: 1.815 | Fields: 9 | Matches: 6\n",
      "4. Docu_Camp (Relation)\n",
      "   Score: 1.802 | Fields: 5 | Matches: 6\n",
      "5. Docu_Docu (Relation)\n",
      "   Score: 1.606 | Fields: 6 | Matches: 5\n",
      "6. Kbas_Kbas (Relation)\n",
      "   Score: 1.605 | Fields: 5 | Matches: 5\n",
      "7. Docu_Kbas (Relation)\n",
      "   Score: 1.605 | Fields: 5 | Matches: 5\n",
      "8. Docu_Vatr (Relation)\n",
      "   Score: 1.405 | Fields: 6 | Matches: 4\n",
      "9. Daqu_Cont (Relation)\n",
      "   Score: 1.405 | Fields: 7 | Matches: 4\n",
      "10. Objv_Cont (Relation)\n",
      "   Score: 1.404 | Fields: 5 | Matches: 4\n",
      "\n",
      "âœ… Ready for SQL Agent Processing\n",
      "\n",
      "ğŸ’¾ SQL Agent data exported to: Mails_ingestion_analysis.json\n",
      "âœ… Ready for SQL Agent: Mails_ingestion_analysis.json\n"
     ]
    }
   ],
   "source": [
    "# Initialize the pipeline\n",
    "pipeline = GenericFileIngestionRAGPipeline(qdrant_client, encoder, \"maxo_vector_store_v2\")\n",
    "\n",
    "# Analyze a file (example with Mails.csv)\n",
    "file_path = r\"C:\\Users\\axel.grille\\Documents\\rules-engine-agent\\Mails.csv\"\n",
    "user_context = \"email correspondence and customer communication tracking\"\n",
    "\n",
    "# Run the complete pipeline\n",
    "results = pipeline.run_complete_pipeline(file_path, user_context)\n",
    "\n",
    "# Display results\n",
    "pipeline.display_results_summary(results)\n",
    "\n",
    "# Export for SQL agent\n",
    "if 'error' not in results:\n",
    "    export_result = pipeline.export_for_sql_agent(results)\n",
    "    if 'success' in export_result:\n",
    "        print(f\"âœ… Ready for SQL Agent: {export_result['output_file']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
