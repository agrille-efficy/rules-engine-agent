{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b51feb1",
   "metadata": {},
   "source": [
    "# Generic File Ingestion RAG Pipeline\n",
    "\n",
    "This notebook contains a comprehensive RAG pipeline for analyzing any data file and finding the top 5 best database tables for data ingestion using semantic search and LLM analysis.\n",
    "\n",
    "## Features:\n",
    "- **Multi-format support**: CSV, Excel, JSON, TSV, TXT files\n",
    "- **Automatic domain detection** from column names\n",
    "- **Context-aware semantic query generation**\n",
    "- **SQL agent optimized output format**\n",
    "- **Confidence scoring for automation decisions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce52fd78",
   "metadata": {},
   "source": [
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f579985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dependencies loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "from uuid import uuid4\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Vector store and embeddings\n",
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_client.models import PointStruct, PayloadSchemaType\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(r'C:\\Users\\axel.grille\\Documents\\rules-engine-agent\\Agent\\.env')\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "QDRANT_URL = os.getenv(\"QDRANT_URL\")\n",
    "QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n",
    "\n",
    "print(\"‚úÖ Dependencies loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2735e096",
   "metadata": {},
   "source": [
    "## Initialize Vector Store and Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c06da1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Clients initialized\n",
      "üìä Available collections: ['maxo_vector_store_v2', 'maxo_vector_store']\n"
     ]
    }
   ],
   "source": [
    "# Initialize OpenAI and Qdrant clients\n",
    "client = OpenAI()\n",
    "encoder = OpenAIEmbeddings()\n",
    "\n",
    "# Initialize Qdrant client\n",
    "qdrant_client = QdrantClient(\n",
    "    url=QDRANT_URL, \n",
    "    api_key=QDRANT_API_KEY,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Clients initialized\")\n",
    "print(f\"üìä Available collections: {[col.name for col in qdrant_client.get_collections().collections]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a591a963",
   "metadata": {},
   "source": [
    "## Database Setup (Run Once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9065c682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Login successful\n",
      "‚úÖ DICO data retrieved\n",
      "‚úÖ Retrieved 409 tables from database schema\n"
     ]
    }
   ],
   "source": [
    "# DICO API call to get database schema\n",
    "import requests\n",
    "\n",
    "def efficy_api_call():\n",
    "    \"\"\"Fetch database schema from Efficy API\"\"\"\n",
    "    session = requests.Session()\n",
    "    \n",
    "    try:\n",
    "        # Login\n",
    "        login_response = session.post(\n",
    "            \"https://sandbox-5.efficytest.cloud/crm/logon\",\n",
    "            headers={\n",
    "                'X-Efficy-Customer': 'SANDBOX05',\n",
    "                'X-Requested-By': 'User',\n",
    "                'X-Requested-With': 'XMLHttpRequest',\n",
    "                'Content-Type': 'application/x-www-form-urlencoded'\n",
    "            },\n",
    "            data='user=paul&password=Eff1cyDemo!'\n",
    "        )\n",
    "        \n",
    "        if login_response.status_code == 200:\n",
    "            print(\"‚úÖ Login successful\")\n",
    "            \n",
    "            # DICO request\n",
    "            dico_response = session.get(\n",
    "                \"https://sandbox-5.efficytest.cloud/crm/system/dico\",\n",
    "                headers={\n",
    "                    'X-Requested-By': 'User',\n",
    "                    'X-Requested-With': 'XMLHttpRequest'\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            if dico_response.status_code == 200:\n",
    "                print(\"‚úÖ DICO data retrieved\")\n",
    "                return dico_response.json()\n",
    "            else:\n",
    "                print(f\"‚ùå DICO request failed: {dico_response.status_code}\")\n",
    "                \n",
    "        else:\n",
    "            print(f\"‚ùå Login failed: {login_response.status_code}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Request error: {e}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Execute the API call (comment out if dico_data already exists)\n",
    "dico_data = efficy_api_call()\n",
    "if dico_data:\n",
    "    print(f\"‚úÖ Retrieved {len(dico_data['data']['tables'])} tables from database schema\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "430cd0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated 286 table chunks\n",
      "‚úÖ Using existing collection: maxo_vector_store_v2\n"
     ]
    }
   ],
   "source": [
    "# Create stable ID function\n",
    "def stable_id(*parts, length=32): \n",
    "    base = '|'.join(str(p) for p in parts)\n",
    "    return hashlib.sha256(base.encode()).hexdigest()[:length]\n",
    "\n",
    "# Generate table chunks for vector store\n",
    "from Agent.RAG.chunk_generator import generate_table_ingestion_chunks\n",
    "\n",
    "if 'table_chunks' not in locals():\n",
    "    table_chunks = generate_table_ingestion_chunks(dico_data)\n",
    "\n",
    "print(f\"‚úÖ Generated {len(table_chunks)} table chunks\")\n",
    "\n",
    "# Create vector store collection\n",
    "collection_name = \"maxo_vector_store_v2\"\n",
    "\n",
    "existing_collections = [col.name for col in qdrant_client.get_collections().collections]\n",
    "if collection_name not in existing_collections: \n",
    "    qdrant_client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=models.VectorParams(\n",
    "            size=len(encoder.embed_query(\"Hello world\")),\n",
    "            distance=models.Distance.COSINE,\n",
    "        ),\n",
    "    )\n",
    "    print(f\"‚úÖ Created collection: {collection_name}\")\n",
    "else:\n",
    "    print(f\"‚úÖ Using existing collection: {collection_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "717f89c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully upserted 286 table chunks\n",
      "üìä Collection now contains 286 total points\n",
      "‚úÖ Payload indexes created\n"
     ]
    }
   ],
   "source": [
    "# Create and upload table points to vector store\n",
    "table_points = []\n",
    "\n",
    "for chunk in table_chunks: \n",
    "    chunk_id = stable_id(\n",
    "        chunk.metadata['chunk_type'],\n",
    "        chunk.metadata['primary_table'],\n",
    "        chunk.metadata['table_code']\n",
    "    )\n",
    "\n",
    "    embedding = encoder.embed_query(chunk.page_content)\n",
    "\n",
    "    point = PointStruct(\n",
    "        id=chunk_id, \n",
    "        vector=embedding,\n",
    "        payload={\n",
    "            'content': chunk.page_content,\n",
    "            'chunk_type': chunk.metadata['chunk_type'],\n",
    "            'primary_table': chunk.metadata['primary_table'],\n",
    "            'table_code': chunk.metadata['table_code'],\n",
    "            'table_kind': chunk.metadata['table_kind'],\n",
    "            'field_count': chunk.metadata['field_count'],\n",
    "            'metadata': chunk.metadata\n",
    "        }\n",
    "    )\n",
    "    table_points.append(point)\n",
    "\n",
    "# Upsert points to Qdrant\n",
    "try:\n",
    "    result = qdrant_client.upsert(\n",
    "        collection_name=collection_name, \n",
    "        points=table_points\n",
    "    )\n",
    "    print(f\"‚úÖ Successfully upserted {len(table_points)} table chunks\")\n",
    "    \n",
    "    collection_info = qdrant_client.get_collection(collection_name)\n",
    "    print(f\"üìä Collection now contains {collection_info.points_count} total points\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during upsert: {e}\")\n",
    "\n",
    "# Create payload indexes for efficient filtering\n",
    "try:\n",
    "    qdrant_client.create_payload_index(\n",
    "        collection_name=collection_name,\n",
    "        field_name=\"chunk_type\",\n",
    "        field_schema=PayloadSchemaType.KEYWORD\n",
    "    )\n",
    "    qdrant_client.create_payload_index(\n",
    "        collection_name=collection_name,\n",
    "        field_name=\"primary_table\",\n",
    "        field_schema=PayloadSchemaType.KEYWORD\n",
    "    )\n",
    "    print(\"‚úÖ Payload indexes created\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Index creation (may already exist): {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0088eeb9",
   "metadata": {},
   "source": [
    "## Generic File Ingestion RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8574d35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GenericFileIngestionRAGPipeline class complete with all methods\n"
     ]
    }
   ],
   "source": [
    "class GenericFileIngestionRAGPipeline:\n",
    "    \"\"\"\n",
    "    Generic RAG pipeline for analyzing any data file and finding the top 5 best\n",
    "    database tables for data ingestion using semantic search and LLM analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, qdrant_client, encoder, collection_name=\"maxo_vector_store_v2\"):\n",
    "        self.qdrant_client = qdrant_client\n",
    "        self.encoder = encoder\n",
    "        self.collection_name = collection_name\n",
    "        self.llm_client = OpenAI()\n",
    "        self.supported_formats = ['.csv', '.xlsx', '.xls', '.json', '.txt', '.tsv']\n",
    "    \n",
    "    def analyze_file_structure(self, file_path):\n",
    "        \"\"\"Analyze any supported file structure and content\"\"\"\n",
    "        try:\n",
    "            if not os.path.exists(file_path):\n",
    "                return {'error': f'File not found: {file_path}'}\n",
    "            \n",
    "            file_extension = os.path.splitext(file_path)[1].lower()\n",
    "            file_name = os.path.basename(file_path)\n",
    "            \n",
    "            if file_extension not in self.supported_formats:\n",
    "                return {'error': f'Unsupported file format: {file_extension}'}\n",
    "            \n",
    "            # Handle different file types\n",
    "            if file_extension == '.csv':\n",
    "                return self._analyze_csv(file_path, file_name)\n",
    "            elif file_extension in ['.xlsx', '.xls']:\n",
    "                return self._analyze_excel(file_path, file_name)\n",
    "            elif file_extension == '.json':\n",
    "                return self._analyze_json(file_path, file_name)\n",
    "            elif file_extension in ['.txt', '.tsv']:\n",
    "                return self._analyze_text(file_path, file_name)\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {'error': f'Failed to analyze file: {str(e)}'}\n",
    "    \n",
    "    def _analyze_csv(self, file_path, file_name):\n",
    "        \"\"\"Analyze CSV files\"\"\"\n",
    "        df = pd.read_csv(file_path)\n",
    "        return self._create_file_analysis(df, file_name, 'CSV')\n",
    "    \n",
    "    def _analyze_excel(self, file_path, file_name):\n",
    "        \"\"\"Analyze Excel files\"\"\"\n",
    "        df = pd.read_excel(file_path)\n",
    "        return self._create_file_analysis(df, file_name, 'Excel')\n",
    "    \n",
    "    def _analyze_json(self, file_path, file_name):\n",
    "        \"\"\"Analyze JSON files\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        if isinstance(data, list) and len(data) > 0 and isinstance(data[0], dict):\n",
    "            df = pd.DataFrame(data)\n",
    "            return self._create_file_analysis(df, file_name, 'JSON Array')\n",
    "        elif isinstance(data, dict):\n",
    "            df = pd.DataFrame([data])\n",
    "            return self._create_file_analysis(df, file_name, 'JSON Object')\n",
    "        else:\n",
    "            return {'error': 'JSON structure not suitable for tabular analysis'}\n",
    "    \n",
    "    def _analyze_text(self, file_path, file_name):\n",
    "        \"\"\"Analyze text/TSV files\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            first_line = f.readline()\n",
    "        \n",
    "        delimiter = '\\t' if '\\t' in first_line else ',' if ',' in first_line else ';'\n",
    "        df = pd.read_csv(file_path, delimiter=delimiter)\n",
    "        return self._create_file_analysis(df, file_name, 'Text/TSV')\n",
    "    \n",
    "    def _create_file_analysis(self, df, file_name, file_type):\n",
    "        \"\"\"Create standardized file analysis from DataFrame\"\"\"\n",
    "        file_info = {\n",
    "            'file_name': file_name,\n",
    "            'file_type': file_type,\n",
    "            'total_rows': len(df),\n",
    "            'total_columns': len(df.columns),\n",
    "            'columns': df.columns.tolist(),\n",
    "            'sample_data': df.head(2).to_dict('records') if len(df) > 0 else []\n",
    "        }\n",
    "        \n",
    "        # Analyze column types and content\n",
    "        column_analysis = {}\n",
    "        for col in df.columns:\n",
    "            column_analysis[col] = {\n",
    "                'dtype': str(df[col].dtype),\n",
    "                'non_null_count': df[col].notna().sum(),\n",
    "                'null_count': df[col].isna().sum(),\n",
    "                'unique_values': df[col].nunique(),\n",
    "                'sample_values': df[col].dropna().head(3).tolist()\n",
    "            }\n",
    "        \n",
    "        file_info['column_analysis'] = column_analysis\n",
    "        return file_info\n",
    "    \n",
    "    def _infer_data_domain(self, columns):\n",
    "        \"Enhanced data domain inference with comprehensive business entity detection\"\n",
    "        columns_lower = [col.lower() for col in columns]\n",
    "        \n",
    "        # Comprehensive domain detection patterns\n",
    "        domain_patterns = {\n",
    "            # Core CRM entities\n",
    "            'leads': [\n",
    "                'lead', 'prospect', 'lead_status', 'source', 'qualification', 'score', 'conversion',\n",
    "                'lead_id', 'prospect_id', 'qualified', 'unqualified', 'mql', 'sql', 'nurture'\n",
    "            ],\n",
    "            'opportunities': [\n",
    "                'opportunity', 'deal', 'pipeline', 'stage', 'probability', 'close_date', 'forecast',\n",
    "                'opp_id', 'deal_id', 'sales_stage', 'win_probability', 'expected_revenue', 'deal_value'\n",
    "            ],\n",
    "            'contacts': [\n",
    "                'contact', 'person', 'individual', 'first_name', 'last_name', 'title', 'relationship',\n",
    "                'contact_id', 'person_id', 'full_name', 'job_title', 'phone', 'mobile', 'email'\n",
    "            ],\n",
    "            'companies': [\n",
    "                'company', 'organization', 'enterprise', 'business', 'industry', 'sector', 'headquarters',\n",
    "                'company_id', 'org_id', 'business_name', 'company_name', 'industry_type', 'company_size'\n",
    "            ],\n",
    "            'activities': [\n",
    "                'activity', 'action', 'event', 'log', 'history', 'timeline', 'interaction',\n",
    "                'activity_id', 'event_id', 'action_type', 'activity_type', 'interaction_type', 'follow_up'\n",
    "            ],\n",
    "            'meetings': [\n",
    "                'meeting', 'appointment', 'schedule', 'calendar', 'attendee', 'agenda', 'duration',\n",
    "                'meeting_id', 'appointment_id', 'scheduled', 'start_time', 'end_time', 'location'\n",
    "            ],\n",
    "            'campaigns': [\n",
    "                'campaign', 'marketing', 'promotion', 'advertising', 'channel', 'target', 'response',\n",
    "                'campaign_id', 'promo_id', 'marketing_campaign', 'campaign_name', 'campaign_type'\n",
    "            ],\n",
    "            'tickets': [\n",
    "                'ticket', 'issue', 'support', 'incident', 'priority', 'resolution', 'escalation',\n",
    "                'ticket_id', 'issue_id', 'support_ticket', 'incident_id', 'case_id', 'help_desk'\n",
    "            ],\n",
    "            'users': [\n",
    "                'user', 'username', 'login', 'profile', 'role', 'permission', 'access',\n",
    "                'user_id', 'account', 'user_name', 'login_name', 'user_role', 'access_level'\n",
    "            ],\n",
    "            \n",
    "            # Extended business domains\n",
    "            'communication': [\n",
    "                'message', 'email', 'mail', 'subject', 'sender', 'recipient', 'date',\n",
    "                'corps du message', 'mail exp√©diteur', 'mail destinataire', 'objet', 'visite_mail'\n",
    "            ],\n",
    "            'sales_orders': ['order', 'product', 'price', 'quantity', 'total', 'invoice', 'payment'],\n",
    "            'financial': ['amount', 'cost', 'revenue', 'budget', 'transaction', 'account', 'currency'],\n",
    "            'hr_employee': ['employee', 'staff', 'salary', 'department', 'position', 'hire', 'manager'],\n",
    "            'inventory': ['item', 'stock', 'warehouse', 'supplier', 'category', 'sku', 'unit'],\n",
    "            'project': ['project', 'task', 'milestone', 'deadline', 'status', 'resource', 'team'],\n",
    "            'logistics': ['shipment', 'delivery', 'tracking', 'carrier', 'destination', 'weight']\n",
    "        }\n",
    "        \n",
    "        # Calculate domain scores with weighted importance\n",
    "        domain_scores = {}\n",
    "        for domain, keywords in domain_patterns.items():\n",
    "            score = 0\n",
    "            matched_keywords = []\n",
    "            \n",
    "            for keyword in keywords:\n",
    "                for col in columns_lower:\n",
    "                    if keyword in col:\n",
    "                        # Weight exact matches higher\n",
    "                        if keyword == col:\n",
    "                            score += 3\n",
    "                        # Weight ID fields higher (strong indicators)\n",
    "                        elif keyword.endswith('_id') and keyword in col:\n",
    "                            score += 2.5\n",
    "                        # Regular substring matches\n",
    "                        else:\n",
    "                            score += 1\n",
    "                        matched_keywords.append(keyword)\n",
    "                        break\n",
    "            \n",
    "            domain_scores[domain] = {\n",
    "                'score': score,\n",
    "                'matched_keywords': list(set(matched_keywords))\n",
    "            }\n",
    "        \n",
    "        # Get best matching domain\n",
    "        best_domain = max(domain_scores, key=lambda x: domain_scores[x]['score']) if domain_scores else 'general'\n",
    "        best_score = domain_scores[best_domain]['score'] if best_domain != 'general' else 0\n",
    "        \n",
    "        # Enhanced domain mapping with confidence indicators\n",
    "        domain_mapping = {\n",
    "            'leads': {\n",
    "                'primary_domain': 'lead management and prospecting', \n",
    "                'business_area': 'sales lead generation', \n",
    "                'data_category': 'leads',\n",
    "                'table_hints': ['lead', 'prospect', 'qualification']\n",
    "            },\n",
    "            'opportunities': {\n",
    "                'primary_domain': 'sales opportunity tracking', \n",
    "                'business_area': 'sales pipeline management', \n",
    "                'data_category': 'opportunities',\n",
    "                'table_hints': ['opportunity', 'deal', 'sales_pipeline']\n",
    "            },\n",
    "            'contacts': {\n",
    "                'primary_domain': 'contact and person management', \n",
    "                'business_area': 'relationship management', \n",
    "                'data_category': 'contacts',\n",
    "                'table_hints': ['contact', 'person', 'individual']\n",
    "            },\n",
    "            'companies': {\n",
    "                'primary_domain': 'company and organization management', \n",
    "                'business_area': 'corporate data management', \n",
    "                'data_category': 'companies',\n",
    "                'table_hints': ['company', 'organization', 'enterprise']\n",
    "            },\n",
    "            'activities': {\n",
    "                'primary_domain': 'activity and event tracking', \n",
    "                'business_area': 'interaction management', \n",
    "                'data_category': 'activities',\n",
    "                'table_hints': ['activity', 'event', 'action', 'visit']\n",
    "            },\n",
    "            'meetings': {\n",
    "                'primary_domain': 'meeting and calendar management', \n",
    "                'business_area': 'scheduling and appointments', \n",
    "                'data_category': 'meetings',\n",
    "                'table_hints': ['meeting', 'appointment', 'schedule']\n",
    "            },\n",
    "            'campaigns': {\n",
    "                'primary_domain': 'marketing campaign management', \n",
    "                'business_area': 'marketing operations', \n",
    "                'data_category': 'campaigns',\n",
    "                'table_hints': ['campaign', 'marketing', 'promotion']\n",
    "            },\n",
    "            'tickets': {\n",
    "                'primary_domain': 'ticketing and support management', \n",
    "                'business_area': 'customer support', \n",
    "                'data_category': 'tickets',\n",
    "                'table_hints': ['ticket', 'support', 'incident']\n",
    "            },\n",
    "            'users': {\n",
    "                'primary_domain': 'user and account management', \n",
    "                'business_area': 'system administration', \n",
    "                'data_category': 'users',\n",
    "                'table_hints': ['user', 'account', 'profile']\n",
    "            },\n",
    "            'communication': {\n",
    "                'primary_domain': 'communication and messaging', \n",
    "                'business_area': 'correspondence', \n",
    "                'data_category': 'communication',\n",
    "                'table_hints': ['mail', 'email', 'message', 'visit']\n",
    "            },\n",
    "            'sales_orders': {'primary_domain': 'sales and order management', 'business_area': 'sales operations', 'data_category': 'transactional', 'table_hints': ['order', 'sale', 'invoice']},\n",
    "            'financial': {'primary_domain': 'financial and accounting', 'business_area': 'finance', 'data_category': 'financial', 'table_hints': ['financial', 'accounting', 'budget']},\n",
    "            'hr_employee': {'primary_domain': 'human resources', 'business_area': 'HR management', 'data_category': 'employee', 'table_hints': ['employee', 'staff', 'hr']},\n",
    "            'inventory': {'primary_domain': 'inventory and stock management', 'business_area': 'supply chain', 'data_category': 'inventory', 'table_hints': ['inventory', 'stock', 'product']},\n",
    "            'project': {'primary_domain': 'project management', 'business_area': 'project operations', 'data_category': 'project', 'table_hints': ['project', 'task', 'milestone']},\n",
    "            'logistics': {'primary_domain': 'logistics and shipping', 'business_area': 'operations', 'data_category': 'logistics', 'table_hints': ['shipment', 'delivery', 'logistics']},\n",
    "            'general': {'primary_domain': 'business data', 'business_area': 'general operations', 'data_category': 'business', 'table_hints': ['data', 'general']}\n",
    "        }\n",
    "        \n",
    "        result = domain_mapping.get(best_domain, domain_mapping['general'])\n",
    "        \n",
    "        # Add detection metadata for debugging and confidence assessment\n",
    "        result['detection_confidence'] = min(best_score / 5.0, 1.0)  # Normalize to 0-1\n",
    "        result['matched_keywords'] = domain_scores.get(best_domain, {}).get('matched_keywords', [])\n",
    "        result['all_scores'] = {k: v['score'] for k, v in domain_scores.items() if v['score'] > 0}\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def generate_semantic_queries(self, file_info, user_context=None):\n",
    "        \"\"\"Generate semantic queries based on file content and optional user context\"\"\"\n",
    "        columns = file_info.get('columns', [])\n",
    "        file_name = file_info.get('file_name', 'data file')\n",
    "        file_type = file_info.get('file_type', 'file')\n",
    "        \n",
    "        domain_hints = self._infer_data_domain(columns)\n",
    "        \n",
    "        # Enhanced email/mail detection\n",
    "        email_indicators = self._detect_email_signals(file_name, columns)\n",
    "        \n",
    "        query_templates = [\n",
    "            f\"database table for storing {domain_hints['primary_domain']} data with fields like {', '.join(columns[:6])}\",\n",
    "            f\"{file_type} data ingestion into relational database with columns {', '.join(columns[:8])}\",\n",
    "            f\"business data management system for {domain_hints['business_area']} information\",\n",
    "            f\"data warehouse table for {domain_hints['data_category']} records and analytics\",\n",
    "            f\"structured data storage for {file_name} content in enterprise database\"\n",
    "        ]\n",
    "        \n",
    "        # If strong email signals detected, prioritize email/mail queries\n",
    "        if email_indicators['is_email_data']:\n",
    "            email_queries = [\n",
    "                f\"mail table for email message storage with sender recipient fields {', '.join(email_indicators['email_columns'])}\",\n",
    "                f\"email communication table for message tracking with mail fields from {file_name}\",\n",
    "                f\"mail system database table for email correspondence management\",\n",
    "                f\"email message storage table with mail content and headers\",\n",
    "                f\"visit mail table for email tracking with fields {', '.join(columns[:4])}\"\n",
    "            ]\n",
    "            # Insert email-specific queries at the beginning for higher priority\n",
    "            query_templates = email_queries + query_templates\n",
    "            print(f\"üîç Enhanced with {len(email_queries)} email-specific queries (confidence: {email_indicators['confidence']:.2f})\")\n",
    "        \n",
    "        if user_context:\n",
    "            context_query = f\"{user_context} with data structure: {', '.join(columns[:10])}\"\n",
    "            query_templates.insert(0, context_query)\n",
    "        \n",
    "        return query_templates\n",
    "    \n",
    "    ####\n",
    "    # REPLACE WITH A CLASSIFIER TO LABEL THE FILE TYPE (EMAIL, APPOINTMENT, LEAD, ...)\n",
    "\n",
    "\n",
    "    # def _detect_email_signals(self, file_name, columns):\n",
    "    #     \"\"\"Enhanced email detection from filename and column patterns\"\"\"\n",
    "    #     email_signals = {\n",
    "    #         'is_email_data': False,\n",
    "    #         'email_columns': [],\n",
    "    #         'confidence': 0.0\n",
    "    #     }\n",
    "        \n",
    "    #     # Check filename for email indicators\n",
    "    #     filename_lower = file_name.lower()\n",
    "    #     filename_email_score = 0\n",
    "    #     if 'mail' in filename_lower:\n",
    "    #         filename_email_score += 3\n",
    "    #     if 'email' in filename_lower:\n",
    "    #         filename_email_score += 3\n",
    "    #     if 'message' in filename_lower:\n",
    "    #         filename_email_score += 2\n",
    "    #     if 'correspondence' in filename_lower:\n",
    "    #         filename_email_score += 2\n",
    "            \n",
    "    #     # Check columns for email-specific patterns (including French terms)\n",
    "    #     columns_lower = [col.lower() for col in columns]\n",
    "    #     email_column_patterns = [\n",
    "    #         'mail', 'email', 'sender', 'recipient', 'subject', 'message', \n",
    "    #         'corps du message', 'mail exp√©diteur', 'mail destinataire',\n",
    "    #         'objet', 'auteur', 'destinataire', 'exp√©diteur', 'visite_mail'\n",
    "    #     ]\n",
    "        \n",
    "    #     column_email_score = 0\n",
    "    #     email_columns = []\n",
    "    #     for col in columns:\n",
    "    #         col_lower = col.lower()\n",
    "    #         for pattern in email_column_patterns:\n",
    "    #             if pattern in col_lower:\n",
    "    #                 column_email_score += 2\n",
    "    #                 email_columns.append(col)\n",
    "    #                 break\n",
    "        \n",
    "    #     total_score = filename_email_score + column_email_score\n",
    "    #     email_signals['confidence'] = min(total_score / 10.0, 1.0)  # Normalize to 0-1\n",
    "    #     email_signals['email_columns'] = email_columns[:4]  # Top 4 email columns\n",
    "        \n",
    "    #     # Consider it email data if confidence > 0.5 or strong filename signal\n",
    "    #     email_signals['is_email_data'] = (\n",
    "    #         email_signals['confidence'] > 0.5 or \n",
    "    #         filename_email_score >= 3\n",
    "    #     )\n",
    "        \n",
    "    #     return email_signals\n",
    "    \n",
    "    def search_relevant_tables(self, queries, top_k=10):\n",
    "        \"\"\"Search for relevant tables using multiple semantic queries\"\"\"\n",
    "        all_results = {}\n",
    "        \n",
    "        for i, query in enumerate(queries):\n",
    "            query_embedding = self.encoder.embed_query(query)\n",
    "            \n",
    "            try:\n",
    "                search_results = self.qdrant_client.query_points(\n",
    "                    collection_name=self.collection_name,\n",
    "                    query=query_embedding,\n",
    "                    query_filter=models.Filter(\n",
    "                        must=[\n",
    "                            models.FieldCondition(\n",
    "                                key=\"chunk_type\",\n",
    "                                match=models.MatchValue(value=\"table_ingestion_profile\")\n",
    "                            )\n",
    "                        ]\n",
    "                    ),\n",
    "                    limit=top_k\n",
    "                )\n",
    "                \n",
    "                for point in search_results.points:\n",
    "                    table_name = point.payload['primary_table']\n",
    "                    \n",
    "                    if table_name not in all_results:\n",
    "                        all_results[table_name] = {\n",
    "                            'table_name': table_name,\n",
    "                            'table_code': point.payload['table_code'],\n",
    "                            'table_kind': point.payload['table_kind'],\n",
    "                            'field_count': point.payload['field_count'],\n",
    "                            'content': point.payload['content'],\n",
    "                            'metadata': point.payload['metadata'],\n",
    "                            'scores': [],\n",
    "                            'queries_matched': []\n",
    "                        }\n",
    "                    \n",
    "                    all_results[table_name]['scores'].append(point.score)\n",
    "                    all_results[table_name]['queries_matched'].append(i+1)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Search error for query {i+1}: {e}\")\n",
    "        \n",
    "        return all_results\n",
    "    \n",
    "    def rank_tables_by_relevance(self, search_results):\n",
    "        \"\"\"Rank tables by multiple relevance criteria\"\"\"\n",
    "        ranked_tables = []\n",
    "        \n",
    "        for table_name, data in search_results.items():\n",
    "            scores = data['scores']\n",
    "            avg_score = sum(scores) / len(scores) if scores else 0\n",
    "            max_score = max(scores) if scores else 0\n",
    "            query_coverage = len(set(data['queries_matched']))\n",
    "            \n",
    "            composite_score = (max_score * 0.4) + (avg_score * 0.4) + (query_coverage * 0.2)\n",
    "            \n",
    "            ranked_tables.append({\n",
    "                'table_name': table_name,\n",
    "                'table_code': data['table_code'],\n",
    "                'table_kind': data['table_kind'],\n",
    "                'field_count': data['field_count'],\n",
    "                'content': data['content'],\n",
    "                'avg_score': avg_score,\n",
    "                'max_score': max_score,\n",
    "                'query_coverage': query_coverage,\n",
    "                'composite_score': composite_score,\n",
    "                'total_matches': len(scores),\n",
    "                'queries_matched': data['queries_matched']\n",
    "            })\n",
    "        \n",
    "        ranked_tables.sort(key=lambda x: x['composite_score'], reverse=True)\n",
    "        return ranked_tables\n",
    "    \n",
    "    def run_complete_pipeline(self, file_path, user_context=None):\n",
    "        \"\"\"Run the complete RAG pipeline for any file type\"\"\"\n",
    "        print(\"=== GENERIC FILE INGESTION RAG PIPELINE ===\")\n",
    "        print(f\"üìÅ Analyzing: {os.path.basename(file_path)}\")\n",
    "        print()\n",
    "        \n",
    "        # Step 1: Analyze file structure\n",
    "        print(\"Step 1: Analyzing file structure...\")\n",
    "        file_info = self.analyze_file_structure(file_path)\n",
    "        if 'error' in file_info:\n",
    "            return file_info\n",
    "        \n",
    "        print(f\"‚úì {file_info['file_type']} file: {file_info['total_columns']} columns, {file_info['total_rows']} rows\")\n",
    "        print(f\"‚úì Detected domain: {self._infer_data_domain(file_info['columns'])['primary_domain']}\")\n",
    "        print()\n",
    "        \n",
    "        # Step 2: Generate semantic search queries\n",
    "        print(\"Step 2: Generating semantic search queries...\")\n",
    "        queries = self.generate_semantic_queries(file_info, user_context)\n",
    "        print(f\"‚úì Generated {len(queries)} queries for database search\")\n",
    "        print()\n",
    "        \n",
    "        # Step 3: Search for relevant tables\n",
    "        print(\"Step 3: Searching for relevant database tables...\")\n",
    "        search_results = self.search_relevant_tables(queries)\n",
    "        print(f\"‚úì Found {len(search_results)} unique tables across all queries\")\n",
    "        print()\n",
    "        \n",
    "        # Step 4: Rank tables by relevance\n",
    "        print(\"Step 4: Ranking tables by relevance...\")\n",
    "        ranked_tables = self.rank_tables_by_relevance(search_results)\n",
    "        print(f\"‚úì Ranked {len(ranked_tables)} tables by composite relevance score\")\n",
    "        print()\n",
    "        \n",
    "        # Compile final results\n",
    "        final_results = {\n",
    "            'file_analysis': file_info,\n",
    "            'inferred_domain': self._infer_data_domain(file_info['columns']),\n",
    "            'user_context': user_context,\n",
    "            'search_queries_used': queries,\n",
    "            'total_tables_found': len(search_results),\n",
    "            'top_10_tables': ranked_tables[:10],  # Changed from top_5_tables to top_10_tables\n",
    "            'ingestion_summary': {\n",
    "                'recommended_table': ranked_tables[0]['table_name'] if ranked_tables else None,\n",
    "                'confidence_level': self._calculate_confidence_level(ranked_tables[0] if ranked_tables else None),\n",
    "                'requires_review': self._requires_review(ranked_tables[0] if ranked_tables else None),\n",
    "                'sql_agent_ready': len(ranked_tables) > 0 and ranked_tables[0]['composite_score'] > 0.6\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return final_results\n",
    "    \n",
    "    def _calculate_confidence_level(self, best_table):\n",
    "        \"\"\"Calculate confidence level for SQL agent\"\"\"\n",
    "        if not best_table:\n",
    "            return 'None'\n",
    "        score = best_table['composite_score']\n",
    "        if score > 0.8:\n",
    "            return 'High'\n",
    "        elif score > 0.6:\n",
    "            return 'Medium'\n",
    "        else:\n",
    "            return 'Low'\n",
    "    \n",
    "    def _requires_review(self, best_table):\n",
    "        \"\"\"Determine if human review is needed before SQL generation\"\"\"\n",
    "        if not best_table:\n",
    "            return True\n",
    "        return best_table['composite_score'] < 0.7\n",
    "    \n",
    "    def display_results_summary(self, results):\n",
    "        \"\"\"Display a formatted summary optimized for SQL agent consumption\"\"\"\n",
    "        if 'error' in results:\n",
    "            print(f\"‚ùå Pipeline Error: {results['error']}\")\n",
    "            return\n",
    "        \n",
    "        file_info = results['file_analysis']\n",
    "        summary = results['ingestion_summary']\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        print(f\"üéØ INGESTION ANALYSIS: {file_info['file_name']}\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"üìä File: {file_info['file_type']} | {file_info['total_rows']} rows | {file_info['total_columns']} columns\")\n",
    "        print(f\"üè∑Ô∏è  Domain: {results['inferred_domain']['primary_domain']}\")\n",
    "        print(f\"üéØ Best Table: {summary['recommended_table']}\")\n",
    "        print(f\"üîç Confidence: {summary['confidence_level']}\")\n",
    "        print(f\"‚ö†Ô∏è  Review Required: {'Yes' if summary['requires_review'] else 'No'}\")\n",
    "        print(f\"ü§ñ SQL Agent Ready: {'Yes' if summary['sql_agent_ready'] else 'No'}\")\n",
    "        print()\n",
    "        \n",
    "        print(\"üìã TOP 10 DATABASE TABLES:\") \n",
    "        for i, table in enumerate(results['top_10_tables'], 1):  \n",
    "            print(f\"{i}. {table['table_name']} ({table['table_kind']})\")\n",
    "            print(f\"   Score: {table['composite_score']:.3f} | Fields: {table['field_count']} | Matches: {table['total_matches']}\")\n",
    "        print()\n",
    "        \n",
    "        if summary['sql_agent_ready']:\n",
    "            print(\"‚úÖ Ready for SQL Agent Processing\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  Requires review before SQL generation\")\n",
    "    \n",
    "    def export_for_sql_agent(self, results, output_file=None):\n",
    "        \"\"\"Export results in format optimized for SQL generation agent\"\"\"\n",
    "        if 'error' in results:\n",
    "            return results\n",
    "        \n",
    "        if not output_file:\n",
    "            file_name = results['file_analysis']['file_name']\n",
    "            base_name = os.path.splitext(file_name)[0]\n",
    "            output_file = f\"{base_name}_ingestion_analysis.json\"\n",
    "        \n",
    "        sql_agent_data = {\n",
    "            'source_file': results['file_analysis']['file_name'],\n",
    "            'file_structure': {\n",
    "                'columns': results['file_analysis']['columns'],\n",
    "                'total_rows': results['file_analysis']['total_rows'],\n",
    "                'column_types': {col: analysis['dtype'] for col, analysis in results['file_analysis']['column_analysis'].items()}\n",
    "            },\n",
    "            'recommended_ingestion': {\n",
    "                'primary_table': results['ingestion_summary']['recommended_table'],\n",
    "                'confidence': results['ingestion_summary']['confidence_level'],\n",
    "                'ready_for_sql': results['ingestion_summary']['sql_agent_ready']\n",
    "            },\n",
    "            'table_options': [\n",
    "                {\n",
    "                    'table_name': table['table_name'],\n",
    "                    'table_code': table['table_code'],\n",
    "                    'relevance_score': round(table['composite_score'], 3),\n",
    "                    'field_count': table['field_count'],\n",
    "                    'table_schema': table['content']\n",
    "                }\n",
    "                for table in results['top_10_tables']  # Changed from top_5_tables to top_10_tables\n",
    "            ],\n",
    "            'generation_timestamp': pd.Timestamp.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(sql_agent_data, f, indent=2, ensure_ascii=False)\n",
    "            print(f\"\\nüíæ SQL Agent data exported to: {output_file}\")\n",
    "            return {'success': True, 'output_file': output_file}\n",
    "        except Exception as e:\n",
    "            return {'error': f'Export failed: {str(e)}'}\n",
    "\n",
    "print(\"‚úÖ GenericFileIngestionRAGPipeline class complete with all methods\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab311cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GENERIC FILE INGESTION RAG PIPELINE ===\n",
      "üìÅ Analyzing: Mails.csv\n",
      "\n",
      "Step 1: Analyzing file structure...\n",
      "‚úì CSV file: 25 columns, 5 rows\n",
      "‚úì Detected domain: communication and messaging\n",
      "\n",
      "Step 2: Generating semantic search queries...\n",
      "üîç Enhanced with 5 email-specific queries (confidence: 1.00)\n",
      "‚úì Generated 11 queries for database search\n",
      "\n",
      "Step 3: Searching for relevant database tables...\n",
      "‚úì Found 48 unique tables across all queries\n",
      "\n",
      "Step 4: Ranking tables by relevance...\n",
      "‚úì Ranked 48 tables by composite relevance score\n",
      "\n",
      "================================================================================\n",
      "üéØ INGESTION ANALYSIS: Mails.csv\n",
      "================================================================================\n",
      "üìä File: CSV | 5 rows | 25 columns\n",
      "üè∑Ô∏è  Domain: communication and messaging\n",
      "üéØ Best Table: Mail\n",
      "üîç Confidence: High\n",
      "‚ö†Ô∏è  Review Required: No\n",
      "ü§ñ SQL Agent Ready: Yes\n",
      "\n",
      "üìã TOP 10 DATABASE TABLES:\n",
      "1. Mail (Entity)\n",
      "   Score: 2.029 | Fields: 31 | Matches: 7\n",
      "2. Contact (Entity)\n",
      "   Score: 1.996 | Fields: 70 | Matches: 7\n",
      "3. Sms (Entity)\n",
      "   Score: 1.814 | Fields: 9 | Matches: 6\n",
      "4. Docu_Docu (Relation)\n",
      "   Score: 1.806 | Fields: 6 | Matches: 6\n",
      "5. Kbas_Kbas (Relation)\n",
      "   Score: 1.406 | Fields: 5 | Matches: 4\n",
      "6. Objective (Entity)\n",
      "   Score: 1.404 | Fields: 25 | Matches: 4\n",
      "7. Docu_Cont (Relation)\n",
      "   Score: 1.404 | Fields: 6 | Matches: 4\n",
      "8. Objv_Cont (Relation)\n",
      "   Score: 1.404 | Fields: 5 | Matches: 4\n",
      "9. Conversation (Entity)\n",
      "   Score: 1.401 | Fields: 18 | Matches: 4\n",
      "10. TimelineSubscription (Entity)\n",
      "   Score: 1.391 | Fields: 9 | Matches: 4\n",
      "\n",
      "‚úÖ Ready for SQL Agent Processing\n",
      "\n",
      "üíæ SQL Agent data exported to: Mails_ingestion_analysis.json\n",
      "‚úÖ Ready for SQL Agent: Mails_ingestion_analysis.json\n"
     ]
    }
   ],
   "source": [
    "# Initialize the pipeline\n",
    "pipeline = GenericFileIngestionRAGPipeline(qdrant_client, encoder, \"maxo_vector_store_v2\")\n",
    "\n",
    "# Analyze a file (example with Mails.csv)\n",
    "file_path = r\"C:\\Users\\axel.grille\\Documents\\rules-engine-agent\\Mails.csv\"\n",
    "user_context = \"email correspondence and customer communication tracking\"\n",
    "\n",
    "# Run the complete pipeline\n",
    "results = pipeline.run_complete_pipeline(file_path, user_context)\n",
    "\n",
    "# Display results\n",
    "pipeline.display_results_summary(results)\n",
    "\n",
    "# Export for SQL agent\n",
    "if 'error' not in results:\n",
    "    export_result = pipeline.export_for_sql_agent(results)\n",
    "    if 'success' in export_result:\n",
    "        print(f\"‚úÖ Ready for SQL Agent: {export_result['output_file']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fff364e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CollectionDescription(name='maxo_vector_store_v2'),\n",
       " CollectionDescription(name='sandbox05_database_schema'),\n",
       " CollectionDescription(name='maxo_vector_store')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdrant_client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)\n",
    "\n",
    "qdrant_client.get_collections().collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b73617d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8469c0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "points=[ScoredPoint(id='6ba91d04-d51e-49ee-3cc1-e86d6e6094d6', version=62, score=0.75811076, payload={'content': \"=== TABLE INGESTION PROFILE: Mail (mail) ===\\nType: Entity Table\\nPurpose: Manage and track email interactions for effective communication and customer relationship management.\\n\\n# SCHEMA & CONSTRAINTS\\nTotal fields: 31\\nRequired fields: (none detected)\\n\\n# FIELD DEFINITIONS\\n- mailKey (None): FOB | NULLABLE\\n- mailInteractionKey (None): FOB | NULLABLE\\n- mailSubject (None): ALPHANUMERIC | NULLABLE\\n- mailSysCreatedDate (None): DATE | NULLABLE\\n- mailSysUpdatedDate (None): DATE | NULLABLE\\n- mailSysCreatedUserKey (None): ALPHANUMERIC | NULLABLE\\n- mailSysUpdatedUserKey (None): ALPHANUMERIC | NULLABLE\\n- mailArchived (None): BOOLEAN | NULLABLE\\n- mailReceivedDate (None): DATE | NULLABLE\\n- mailMailFrom (None): ALPHANUMERIC | NULLABLE\\n- mailMailTo (None): MEMO | NULLABLE\\n- mailMailCc (None): MEMO | NULLABLE\\n- mailBody (None): BLOB | NULLABLE\\n- mailHtmlBody (None): BLOB | NULLABLE\\n- mailAttachments (None): NUMBER | NULLABLE\\n- mailTags (None): REFERENCE | NULLABLE\\n- mailFolder (None): REFERENCE | NULLABLE\\n- mailFromOwa (None): BOOLEAN | NULLABLE\\n- mailMailBcc (None): MEMO | NULLABLE\\n- mailSecurityChanges (None): ALPHANUMERIC | NULLABLE\\n- mailKeepSecurity (None): BOOLEAN | NULLABLE\\n- mailUploadState (None): ALPHANUMERIC | NULLABLE\\n- mailGmailId (None): ALPHANUMERIC | NULLABLE\\n- mailOutlookMailID (None): ALPHANUMERIC | NULLABLE\\n- mailTemplateKey (None): ALPHANUMERIC | NULLABLE\\n- mailCategory (None): REFERENCE | NULLABLE\\n- mailChannel (None): REFERENCE | NULLABLE\\n- mailPriority (None): REFERENCE | NULLABLE\\n- mailStatus (None): REFERENCE | NULLABLE\\n- mailReadOnly (None): BOOLEAN | NULLABLE\\n- mailSummary (None): MEMO | NULLABLE\\n\\n# VALIDATION RULES\\n- All REQUIRED fields must be provided.\\n- Respect maximum sizes for VARCHAR-like fields.\\n- Ensure data types (numeric/date/text) align with field expectations.\\n\\n# FOREIGN KEY RELATIONSHIPS\\n(No foreign keys detected)\\n\\n# COMMON DATA MAPPING EXAMPLES\\nUser source column 'Full Name' -> map to field containing keyword: name\\nUser source column 'EmailAddress' -> map to field containing: email\\nUser source column 'Phone' -> map to field containing: phone\\nIf ambiguity (e.g., multiple phone fields), flag for human review instead of guessing.\\n\\n# INSERTION ORDER GUIDANCE\\nThis entity can be inserted independently (no FK dependencies detected).\\n\\n# RISK / AMBIGUITY FLAGS\\n- No required fields detected: verify if primary key is auto-generated.\\n\", 'chunk_type': 'table_ingestion_profile', 'primary_table': 'Mail', 'table_code': 'mail', 'table_kind': 'Entity', 'field_count': 31, 'metadata': {'chunk_type': 'table_ingestion_profile', 'primary_table': 'Mail', 'table_code': 'mail', 'table_kind': 'Entity', 'field_count': 31}}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='3c634c2a-7bb4-7119-ec3f-63cfeae61adf', version=62, score=0.73824716, payload={'content': \"=== TABLE INGESTION PROFILE: Sms (smss) ===\\nType: Entity Table\\nPurpose: Track and manage SMS interactions, including status and timestamps for effective communication analysis.\\n\\n# SCHEMA & CONSTRAINTS\\nTotal fields: 9\\nRequired fields: (none detected)\\n\\n# FIELD DEFINITIONS\\n- smssKey (None): FOB | NULLABLE\\n- smssInteractionKey (None): FOB | NULLABLE\\n- smssSentDate (None): DATE | NULLABLE\\n- smssSentStatus (None): ALPHANUMERIC | NULLABLE\\n- smssSysCreatedDate (None): DATE | NULLABLE\\n- smssSysUpdatedDate (None): DATE | NULLABLE\\n- smssSysCreatedUserKey (None): ALPHANUMERIC | NULLABLE\\n- smssSysUpdatedUserKey (None): ALPHANUMERIC | NULLABLE\\n- smssMemo (None): MEMO | NULLABLE\\n\\n# VALIDATION RULES\\n- All REQUIRED fields must be provided.\\n- Respect maximum sizes for VARCHAR-like fields.\\n- Ensure data types (numeric/date/text) align with field expectations.\\n\\n# FOREIGN KEY RELATIONSHIPS\\n(No foreign keys detected)\\n\\n# COMMON DATA MAPPING EXAMPLES\\nUser source column 'Full Name' -> map to field containing keyword: name\\nUser source column 'EmailAddress' -> map to field containing: email\\nUser source column 'Phone' -> map to field containing: phone\\nIf ambiguity (e.g., multiple phone fields), flag for human review instead of guessing.\\n\\n# INSERTION ORDER GUIDANCE\\nThis entity can be inserted independently (no FK dependencies detected).\\n\\n# RISK / AMBIGUITY FLAGS\\n- No required fields detected: verify if primary key is auto-generated.\\n\", 'chunk_type': 'table_ingestion_profile', 'primary_table': 'Sms', 'table_code': 'smss', 'table_kind': 'Entity', 'field_count': 9, 'metadata': {'chunk_type': 'table_ingestion_profile', 'primary_table': 'Sms', 'table_code': 'smss', 'table_kind': 'Entity', 'field_count': 9}}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='f6a94d7a-4767-6176-ec43-7c5629f39d79', version=62, score=0.7221384, payload={'content': \"=== TABLE INGESTION PROFILE: KnowledgeBaseComment (kbac) ===\\nType: Entity Table\\nPurpose: Store and manage user comments on knowledge base articles for improved support and information sharing.\\n\\n# SCHEMA & CONSTRAINTS\\nTotal fields: 14\\nRequired fields: (none detected)\\n\\n# FIELD DEFINITIONS\\n- kbacKey (None): FOB | NULLABLE\\n- kbacSubject (None): ALPHANUMERIC | NULLABLE\\n- kbacMemo (None): MEMO | NULLABLE\\n- kbacUserKey (None): FOB | NULLABLE\\n- kbacType (None): REFERENCE | NULLABLE\\n- kbacKnowledgeBaseKey (None): FOB | NULLABLE\\n- kbacStatus (None): REFERENCE | NULLABLE\\n- kbacLanguage (None): REFERENCE | NULLABLE\\n- kbacSysCreatedUserKey (None): FOB | NULLABLE\\n- kbacSysUpdatedUserKey (None): FOB | NULLABLE\\n- kbacSysCreatedDate (None): DATE | NULLABLE\\n- kbacSysUpdatedDate (None): DATE | NULLABLE\\n- kbacDocumentKey (None): FOB | NULLABLE\\n- kbacContactKey (None): FOB | NULLABLE\\n\\n# VALIDATION RULES\\n- All REQUIRED fields must be provided.\\n- Respect maximum sizes for VARCHAR-like fields.\\n- Ensure data types (numeric/date/text) align with field expectations.\\n\\n# FOREIGN KEY RELATIONSHIPS\\n(No foreign keys detected)\\n\\n# COMMON DATA MAPPING EXAMPLES\\nUser source column 'Full Name' -> map to field containing keyword: name\\nUser source column 'EmailAddress' -> map to field containing: email\\nUser source column 'Phone' -> map to field containing: phone\\nIf ambiguity (e.g., multiple phone fields), flag for human review instead of guessing.\\n\\n# INSERTION ORDER GUIDANCE\\nThis entity can be inserted independently (no FK dependencies detected).\\n\\n# RISK / AMBIGUITY FLAGS\\n- No required fields detected: verify if primary key is auto-generated.\\n\", 'chunk_type': 'table_ingestion_profile', 'primary_table': 'KnowledgeBaseComment', 'table_code': 'kbac', 'table_kind': 'Entity', 'field_count': 14, 'metadata': {'chunk_type': 'table_ingestion_profile', 'primary_table': 'KnowledgeBaseComment', 'table_code': 'kbac', 'table_kind': 'Entity', 'field_count': 14}}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='1bfee868-848f-069f-d638-8d03b9a7b933', version=62, score=0.7175395, payload={'content': \"=== TABLE INGESTION PROFILE: KnowledgeBase (kbas) ===\\nType: Entity Table\\nPurpose: Centralize and manage knowledge articles for improved access, evaluation, and dissemination within the organization.\\n\\n# SCHEMA & CONSTRAINTS\\nTotal fields: 27\\nRequired fields: (none detected)\\n\\n# FIELD DEFINITIONS\\n- kbasKey (None): FOB | NULLABLE\\n- kbasTags (None): REFERENCE | NULLABLE\\n- kbasSource (None): REFERENCE | NULLABLE\\n- kbasStatus (None): REFERENCE | NULLABLE\\n- kbasPublicationChannel (None): REFERENCE | NULLABLE\\n- kbasExpirationDate (None): DATE | NULLABLE\\n- kbasPublicationDate (None): DATE | NULLABLE\\n- kbasIsHighlighted (None): BOOLEAN | NULLABLE\\n- kbasName (None): ALPHANUMERIC | NULLABLE\\n- kbasMemo (None): MEMO | NULLABLE\\n- kbasAverageScore (None): NUMERIC | NULLABLE\\n- kbasNumberOfVotes (None): INTEGER | NULLABLE\\n- kbasStatusChangeDate (None): DATE | NULLABLE\\n- kbasAudience (None): REFERENCE | NULLABLE\\n- kbasVersion (None): INTEGER | NULLABLE\\n- kbasSysCreatedUserKey (None): FOB | NULLABLE\\n- kbasSysUpdatedUserKey (None): FOB | NULLABLE\\n- kbasSysCreatedDate (None): DATE | NULLABLE\\n- kbasSysUpdatedDate (None): DATE | NULLABLE\\n- kbasIsRemoved (None): BOOLEAN | NULLABLE\\n- kbasRequestQualificationKey (None): FOB | NULLABLE\\n- kbasCode (None): ALPHANUMERIC | NULLABLE\\n- kbasConsult (None): INTEGER | NULLABLE\\n- kbasMemoFormatted (None): BOOLEAN | NULLABLE\\n- kbasArchived (None): BOOLEAN | NULLABLE\\n- kbasLanguage (None): REFERENCE | NULLABLE\\n- kbasOriginalKey (None): FOB | NULLABLE\\n\\n# VALIDATION RULES\\n- All REQUIRED fields must be provided.\\n- Respect maximum sizes for VARCHAR-like fields.\\n- Ensure data types (numeric/date/text) align with field expectations.\\n\\n# FOREIGN KEY RELATIONSHIPS\\n(No foreign keys detected)\\n\\n# COMMON DATA MAPPING EXAMPLES\\nUser source column 'Full Name' -> map to field containing keyword: name\\nUser source column 'EmailAddress' -> map to field containing: email\\nUser source column 'Phone' -> map to field containing: phone\\nIf ambiguity (e.g., multiple phone fields), flag for human review instead of guessing.\\n\\n# INSERTION ORDER GUIDANCE\\nThis entity can be inserted independently (no FK dependencies detected).\\n\\n# RISK / AMBIGUITY FLAGS\\n- No required fields detected: verify if primary key is auto-generated.\\n\", 'chunk_type': 'table_ingestion_profile', 'primary_table': 'KnowledgeBase', 'table_code': 'kbas', 'table_kind': 'Entity', 'field_count': 27, 'metadata': {'chunk_type': 'table_ingestion_profile', 'primary_table': 'KnowledgeBase', 'table_code': 'kbas', 'table_kind': 'Entity', 'field_count': 27}}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='1fe4a982-f63e-b57d-6d45-50ca3c4a6ce4', version=62, score=0.7173611, payload={'content': \"=== TABLE INGESTION PROFILE: Note (note) ===\\nType: Entity Table\\nPurpose: Store and manage user-generated notes with metadata for tracking and categorization within the CRM/ERP system.\\n\\n# SCHEMA & CONSTRAINTS\\nTotal fields: 13\\nRequired fields: (none detected)\\n\\n# FIELD DEFINITIONS\\n- noteKey (None): FOB | NULLABLE\\n- noteSysUpdatedDate (None): DATE | NULLABLE\\n- noteSysCreatedUserKey (None): FOB | NULLABLE\\n- noteSysUpdatedUserKey (None): FOB | NULLABLE\\n- noteSysCreatedDate (None): DATE | NULLABLE\\n- noteSubject (None): ALPHANUMERIC | NULLABLE\\n- noteMemo (None): BLOB | NULLABLE\\n- noteMemoFormatted (None): BOOLEAN | NULLABLE\\n- notePinned (None): BOOLEAN | NULLABLE\\n- noteStatus (None): REFERENCE | NULLABLE\\n- noteChannel (None): REFERENCE | NULLABLE\\n- noteCategory (None): REFERENCE | NULLABLE\\n- noteInteractionKey (None): FOB | NULLABLE\\n\\n# VALIDATION RULES\\n- All REQUIRED fields must be provided.\\n- Respect maximum sizes for VARCHAR-like fields.\\n- Ensure data types (numeric/date/text) align with field expectations.\\n\\n# FOREIGN KEY RELATIONSHIPS\\n(No foreign keys detected)\\n\\n# COMMON DATA MAPPING EXAMPLES\\nUser source column 'Full Name' -> map to field containing keyword: name\\nUser source column 'EmailAddress' -> map to field containing: email\\nUser source column 'Phone' -> map to field containing: phone\\nIf ambiguity (e.g., multiple phone fields), flag for human review instead of guessing.\\n\\n# INSERTION ORDER GUIDANCE\\nThis entity can be inserted independently (no FK dependencies detected).\\n\\n# RISK / AMBIGUITY FLAGS\\n- No required fields detected: verify if primary key is auto-generated.\\n\", 'chunk_type': 'table_ingestion_profile', 'primary_table': 'Note', 'table_code': 'note', 'table_kind': 'Entity', 'field_count': 13, 'metadata': {'chunk_type': 'table_ingestion_profile', 'primary_table': 'Note', 'table_code': 'note', 'table_kind': 'Entity', 'field_count': 13}}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='284be22d-d4c4-6fb8-4f38-5f382533e127', version=62, score=0.7172866, payload={'content': \"=== TABLE INGESTION PROFILE: Contact (cont) ===\\nType: Entity Table\\nPurpose: Manage and track customer contact information and interactions for effective relationship management.\\n\\n# SCHEMA & CONSTRAINTS\\nTotal fields: 70\\nRequired fields: (none detected)\\n\\n# FIELD DEFINITIONS\\n- contKey (None): FOB | NULLABLE\\n- contName (None): ALPHANUMERIC | NULLABLE\\n- contFirstName (None): ALPHANUMERIC | NULLABLE\\n- contSysCreatedDate (None): DATE | NULLABLE\\n- contSysUpdatedDate (None): DATE | NULLABLE\\n- contSysCreatedUserKey (None): ALPHANUMERIC | NULLABLE\\n- contSysUpdatedUserKey (None): ALPHANUMERIC | NULLABLE\\n- contArchived (None): BOOLEAN | NULLABLE\\n- contGender (None): ALPHANUMERIC | NULLABLE\\n- contLanguage (None): REFERENCE | NULLABLE\\n- contTitle (None): ALPHANUMERIC | NULLABLE\\n- contSuffix (None): ALPHANUMERIC | NULLABLE\\n- contMiddleName (None): ALPHANUMERIC | NULLABLE\\n- contPrefix (None): ALPHANUMERIC | NULLABLE\\n- contCivility (None): ALPHANUMERIC | NULLABLE\\n- contOutlookCategory (None): ALPHANUMERIC | NULLABLE\\n- contBirthdayDate (None): DATE | NULLABLE\\n- contPersonAddressDefault (None): BOOLEAN | NULLABLE\\n- contPhone (None): ALPHANUMERIC | NULLABLE\\n- contMobilePhone (None): ALPHANUMERIC | NULLABLE\\n- contPhone3 (None): ALPHANUMERIC | NULLABLE\\n- contPhone4 (None): ALPHANUMERIC | NULLABLE\\n- contFax (None): ALPHANUMERIC | NULLABLE\\n- contFax2 (None): ALPHANUMERIC | NULLABLE\\n- contWebsite (None): ALPHANUMERIC | NULLABLE\\n- contWebsite2 (None): ALPHANUMERIC | NULLABLE\\n- contMail (None): ALPHANUMERIC | NULLABLE\\n- contMail2 (None): ALPHANUMERIC | NULLABLE\\n- contMail3 (None): ALPHANUMERIC | NULLABLE\\n- contMemo (None): BLOB | NULLABLE\\n- contFullName (None): ALPHANUMERIC | NULLABLE\\n- contMailingCode (None): ALPHANUMERIC | NULLABLE\\n- contMemoFormatted (None): BOOLEAN | NULLABLE\\n- contGlobalRanking (None): INTEGER | NULLABLE\\n- contUserRanking (None): INTEGER | NULLABLE\\n- contExtranetCompanyKey (None): ALPHANUMERIC | NULLABLE\\n- contRobinson (None): BOOLEAN | NULLABLE\\n- contSysUsedDate (None): DATE | NULLABLE\\n- contOutlookMemo (None): ALPHANUMERIC | NULLABLE\\n- contPicture (None): BLOB | NULLABLE\\n- contSkypeIdentity (None): ALPHANUMERIC | NULLABLE\\n- contSocialFacebookUrl (None): ALPHANUMERIC | NULLABLE\\n- contSocialLinkedinUrl (None): ALPHANUMERIC | NULLABLE\\n- contSocialLinkedinId (None): ALPHANUMERIC | NULLABLE\\n- contMailStatus (None): INTEGER | NULLABLE\\n- contSynchro (None): BOOLEAN | NULLABLE\\n- contXingIdentity (None): ALPHANUMERIC | NULLABLE\\n- contPictureUrl (None): ALPHANUMERIC | NULLABLE\\n- contBanner (None): ALPHANUMERIC | NULLABLE\\n- contTags (None): REFERENCE | NULLABLE\\n- contMarketingEmails (None): ALPHANUMERIC | NULLABLE\\n- contNationalNumber (None): ALPHANUMERIC | NULLABLE\\n- contNationality (None): ALPHANUMERIC | NULLABLE\\n- contBirthplace (None): ALPHANUMERIC | NULLABLE\\n- contManager (None): ALPHANUMERIC | NULLABLE\\n- contSocialTwitterUrl (None): ALPHANUMERIC | NULLABLE\\n- contLanguagesSpoken (None): REFERENCE | NULLABLE\\n- contCustomerReference (None): ALPHANUMERIC | NULLABLE\\n- contOriginKey (None): FOB | NULLABLE\\n- contSubscriptions (None): REFERENCE | NULLABLE\\n- contTimeZone (None): REFERENCE | NULLABLE\\n- contDataPrivacyLastMoveDate (None): DATE | NULLABLE\\n- contStatus (None): ALPHANUMERIC | NULLABLE\\n- contAISummaryOutdated (None): BOOLEAN | NULLABLE\\n- contAISummary (None): MEMO | NULLABLE\\n- contPartitionKey (None): FOB | NULLABLE\\n- contInvoiceable (None): BOOLEAN | NULLABLE\\n- contExtranetAccessSubsidiaries (None): BOOLEAN | NULLABLE\\n- contDataPrivacyConsentAlert (None): REFERENCE | NULLABLE\\n- contExternalId (None): ALPHANUMERIC | NULLABLE\\n\\n# VALIDATION RULES\\n- All REQUIRED fields must be provided.\\n- Respect maximum sizes for VARCHAR-like fields.\\n- Ensure data types (numeric/date/text) align with field expectations.\\n\\n# FOREIGN KEY RELATIONSHIPS\\n(No foreign keys detected)\\n\\n# COMMON DATA MAPPING EXAMPLES\\nUser source column 'Full Name' -> map to field containing keyword: name\\nUser source column 'EmailAddress' -> map to field containing: email\\nUser source column 'Phone' -> map to field containing: phone\\nIf ambiguity (e.g., multiple phone fields), flag for human review instead of guessing.\\n\\n# INSERTION ORDER GUIDANCE\\nThis entity can be inserted independently (no FK dependencies detected).\\n\\n# RISK / AMBIGUITY FLAGS\\n- No required fields detected: verify if primary key is auto-generated.\\n\", 'chunk_type': 'table_ingestion_profile', 'primary_table': 'Contact', 'table_code': 'cont', 'table_kind': 'Entity', 'field_count': 70, 'metadata': {'chunk_type': 'table_ingestion_profile', 'primary_table': 'Contact', 'table_code': 'cont', 'table_kind': 'Entity', 'field_count': 70}}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='7a5c8a3b-52e4-a383-1dd9-1120a2c6db80', version=62, score=0.7151817, payload={'content': \"=== TABLE INGESTION PROFILE: Document (docu) ===\\nType: Entity Table\\nPurpose: Manage and track documents, including creation, updates, and archival status for efficient record-keeping.\\n\\n# SCHEMA & CONSTRAINTS\\nTotal fields: 31\\nRequired fields: (none detected)\\n\\n# FIELD DEFINITIONS\\n- docuKey (None): FOB | NULLABLE\\n- docuName (None): ALPHANUMERIC | NULLABLE\\n- docuSysCreatedDate (None): DATE | NULLABLE\\n- docuSysUpdatedDate (None): DATE | NULLABLE\\n- docuSysCreatedUserKey (None): ALPHANUMERIC | NULLABLE\\n- docuSysUpdatedUserKey (None): ALPHANUMERIC | NULLABLE\\n- docuArchived (None): BOOLEAN | NULLABLE\\n- docuUniqueReference (None): ALPHANUMERIC | NULLABLE\\n- docuUniqueRefIsFixed (None): BOOLEAN | NULLABLE\\n- docuEmail (None): BOOLEAN | NULLABLE\\n- docuAttachments (None): INTEGER | NULLABLE\\n- docuMemo (None): BLOB | NULLABLE\\n- docuUsers (None): ALPHANUMERIC | NULLABLE\\n- docuMemoFormatted (None): BOOLEAN | NULLABLE\\n- docuSysUsedDate (None): DATE | NULLABLE\\n- docuSecurityChanges (None): ALPHANUMERIC | NULLABLE\\n- docuKeepSecurity (None): BOOLEAN | NULLABLE\\n- docuUploadState (None): ALPHANUMERIC | NULLABLE\\n- docuPreventSharing (None): BOOLEAN | NULLABLE\\n- docuCompany (None): ALPHANUMERIC | NULLABLE\\n- docuContact (None): ALPHANUMERIC | NULLABLE\\n- docuProject (None): ALPHANUMERIC | NULLABLE\\n- docuOpportunity (None): ALPHANUMERIC | NULLABLE\\n- docuRequest (None): ALPHANUMERIC | NULLABLE\\n- docuCampaign (None): ALPHANUMERIC | NULLABLE\\n- docuCategory (None): ALPHANUMERIC | NULLABLE\\n- docuPartitionKey (None): ALPHANUMERIC | NULLABLE\\n- docuType (None): ALPHANUMERIC | NULLABLE\\n- docuFileSize (None): INTEGER | NULLABLE\\n- docuFileExtension (None): ALPHANUMERIC | NULLABLE\\n- docuExternalId (None): ALPHANUMERIC | NULLABLE\\n\\n# VALIDATION RULES\\n- All REQUIRED fields must be provided.\\n- Respect maximum sizes for VARCHAR-like fields.\\n- Ensure data types (numeric/date/text) align with field expectations.\\n\\n# FOREIGN KEY RELATIONSHIPS\\n(No foreign keys detected)\\n\\n# COMMON DATA MAPPING EXAMPLES\\nUser source column 'Full Name' -> map to field containing keyword: name\\nUser source column 'EmailAddress' -> map to field containing: email\\nUser source column 'Phone' -> map to field containing: phone\\nIf ambiguity (e.g., multiple phone fields), flag for human review instead of guessing.\\n\\n# INSERTION ORDER GUIDANCE\\nThis entity can be inserted independently (no FK dependencies detected).\\n\\n# RISK / AMBIGUITY FLAGS\\n- No required fields detected: verify if primary key is auto-generated.\\n\", 'chunk_type': 'table_ingestion_profile', 'primary_table': 'Document', 'table_code': 'docu', 'table_kind': 'Entity', 'field_count': 31, 'metadata': {'chunk_type': 'table_ingestion_profile', 'primary_table': 'Document', 'table_code': 'docu', 'table_kind': 'Entity', 'field_count': 31}}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='08bed4e5-2a57-e180-747a-cf52f5b68990', version=62, score=0.7144745, payload={'content': \"=== TABLE INGESTION PROFILE: TimeBasedQueue (tbqe) ===\\nType: Entity Table\\nPurpose: Manage and track time-sensitive tasks and actions within the CRM/ERP system.\\n\\n# SCHEMA & CONSTRAINTS\\nTotal fields: 17\\nRequired fields: (none detected)\\n\\n# FIELD DEFINITIONS\\n- tbqeKey (None): FOB | NULLABLE\\n- tbqeSysUpdatedDate (None): DATE | NULLABLE\\n- tbqeSysCreatedUserKey (None): FOB | NULLABLE\\n- tbqeSysUpdatedUserKey (None): FOB | NULLABLE\\n- tbqeSysCreatedDate (None): DATE | NULLABLE\\n- tbqeType (None): REFERENCE | NULLABLE\\n- tbqeStatus (None): REFERENCE | NULLABLE\\n- tbqeHandler (None): ALPHANUMERIC | NULLABLE\\n- tbqeEntityTable (None): INTEGER | NULLABLE\\n- tbqeEntityKey (None): FOB | NULLABLE\\n- tbqeActionKey (None): FOB | NULLABLE\\n- tbqeTriggerUserKey (None): FOB | NULLABLE\\n- tbqeScheduledDate (None): DATE | NULLABLE\\n- tbqeTriggerDate (None): DATE | NULLABLE\\n- tbqeTriggerContext (None): MEMO | NULLABLE\\n- tbqeExecDate (None): DATE | NULLABLE\\n- tbqeExecErrMessage (None): MEMO | NULLABLE\\n\\n# VALIDATION RULES\\n- All REQUIRED fields must be provided.\\n- Respect maximum sizes for VARCHAR-like fields.\\n- Ensure data types (numeric/date/text) align with field expectations.\\n\\n# FOREIGN KEY RELATIONSHIPS\\n(No foreign keys detected)\\n\\n# COMMON DATA MAPPING EXAMPLES\\nUser source column 'Full Name' -> map to field containing keyword: name\\nUser source column 'EmailAddress' -> map to field containing: email\\nUser source column 'Phone' -> map to field containing: phone\\nIf ambiguity (e.g., multiple phone fields), flag for human review instead of guessing.\\n\\n# INSERTION ORDER GUIDANCE\\nThis entity can be inserted independently (no FK dependencies detected).\\n\\n# RISK / AMBIGUITY FLAGS\\n- No required fields detected: verify if primary key is auto-generated.\\n\", 'chunk_type': 'table_ingestion_profile', 'primary_table': 'TimeBasedQueue', 'table_code': 'tbqe', 'table_kind': 'Entity', 'field_count': 17, 'metadata': {'chunk_type': 'table_ingestion_profile', 'primary_table': 'TimeBasedQueue', 'table_code': 'tbqe', 'table_kind': 'Entity', 'field_count': 17}}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='a2ccc256-3258-e2e6-0c88-2fb03c272ea5', version=62, score=0.71421593, payload={'content': \"=== TABLE INGESTION PROFILE: Conversation (conv) ===\\nType: Entity Table\\nPurpose: Track and manage customer interactions, ensuring effective communication and documentation across various channels and categories.\\n\\n# SCHEMA & CONSTRAINTS\\nTotal fields: 18\\nRequired fields: (none detected)\\n\\n# FIELD DEFINITIONS\\n- convKey (None): FOB | NULLABLE\\n- convSysUpdatedDate (None): DATE | NULLABLE\\n- convSysCreatedUserKey (None): ALPHANUMERIC | NULLABLE\\n- convSysUpdatedUserKey (None): ALPHANUMERIC | NULLABLE\\n- convSysCreatedDate (None): DATE | NULLABLE\\n- convInteractionKey (None): FOB | NULLABLE\\n- convMemo (None): BLOB | NULLABLE\\n- convSubject (None): ALPHANUMERIC | NULLABLE\\n- convChannel (None): REFERENCE | NULLABLE\\n- convCategory (None): REFERENCE | NULLABLE\\n- convMemoFormatted (None): BOOLEAN | NULLABLE\\n- convIsHighlighted (None): BOOLEAN | NULLABLE\\n- convRating (None): NUMBER | NULLABLE\\n- convAuthorContactKey (None): FOB | NULLABLE\\n- convReplyToKey (None): FOB | NULLABLE\\n- convMailFrom (None): ALPHANUMERIC | NULLABLE\\n- convMailTo (None): MEMO | NULLABLE\\n- convMailCc (None): MEMO | NULLABLE\\n\\n# VALIDATION RULES\\n- All REQUIRED fields must be provided.\\n- Respect maximum sizes for VARCHAR-like fields.\\n- Ensure data types (numeric/date/text) align with field expectations.\\n\\n# FOREIGN KEY RELATIONSHIPS\\n(No foreign keys detected)\\n\\n# COMMON DATA MAPPING EXAMPLES\\nUser source column 'Full Name' -> map to field containing keyword: name\\nUser source column 'EmailAddress' -> map to field containing: email\\nUser source column 'Phone' -> map to field containing: phone\\nIf ambiguity (e.g., multiple phone fields), flag for human review instead of guessing.\\n\\n# INSERTION ORDER GUIDANCE\\nThis entity can be inserted independently (no FK dependencies detected).\\n\\n# RISK / AMBIGUITY FLAGS\\n- No required fields detected: verify if primary key is auto-generated.\\n\", 'chunk_type': 'table_ingestion_profile', 'primary_table': 'Conversation', 'table_code': 'conv', 'table_kind': 'Entity', 'field_count': 18, 'metadata': {'chunk_type': 'table_ingestion_profile', 'primary_table': 'Conversation', 'table_code': 'conv', 'table_kind': 'Entity', 'field_count': 18}}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='34021a83-671c-840c-1c06-2dc1d8a45278', version=62, score=0.71312225, payload={'content': \"=== TABLE INGESTION PROFILE: Lead (lead) ===\\nType: Entity Table\\nPurpose: Manage and track potential customers' information and interactions for effective sales and marketing efforts.\\n\\n# SCHEMA & CONSTRAINTS\\nTotal fields: 60\\nRequired fields: (none detected)\\n\\n# FIELD DEFINITIONS\\n- leadKey (None): FOB | NULLABLE\\n- leadSysUpdatedDate (None): DATE | NULLABLE\\n- leadSysCreatedUserKey (None): FOB | NULLABLE\\n- leadSysUpdatedUserKey (None): FOB | NULLABLE\\n- leadSysCreatedDate (None): DATE | NULLABLE\\n- leadEmail (None): ALPHANUMERIC | NULLABLE\\n- leadLastName (None): ALPHANUMERIC | NULLABLE\\n- leadFirstName (None): ALPHANUMERIC | NULLABLE\\n- leadCompanyName (None): ALPHANUMERIC | NULLABLE\\n- leadPhone (None): ALPHANUMERIC | NULLABLE\\n- leadMobilePhone (None): ALPHANUMERIC | NULLABLE\\n- leadSector (None): ALPHANUMERIC | NULLABLE\\n- leadEmployeeNumber (None): INTEGER | NULLABLE\\n- leadTurnover (None): REFERENCE | NULLABLE\\n- leadAmount (None): NUMERIC | NULLABLE\\n- leadClosingDate (None): DATE | NULLABLE\\n- leadConversionDate (None): DATE | NULLABLE\\n- leadDataPrivacyLastMoveDate (None): DATE | NULLABLE\\n- leadSource (None): REFERENCE | NULLABLE\\n- leadEngagement (None): REFERENCE | NULLABLE\\n- leadStatus (None): REFERENCE | NULLABLE\\n- leadTransformDeleteAfter (None): BOOLEAN | NULLABLE\\n- leadTransformAutomatic (None): BOOLEAN | NULLABLE\\n- leadTransformPolicyCompany (None): REFERENCE | NULLABLE\\n- leadTransformPolicyContact (None): REFERENCE | NULLABLE\\n- leadTransformCreateCompany (None): BOOLEAN | NULLABLE\\n- leadTransformCreateContact (None): BOOLEAN | NULLABLE\\n- leadTransformCreateEvent (None): BOOLEAN | NULLABLE\\n- leadTransformStatus (None): REFERENCE | NULLABLE\\n- leadTransformUpdatePolicy (None): REFERENCE | NULLABLE\\n- leadTransformCreateExtranet (None): BOOLEAN | NULLABLE\\n- leadMemo (None): BLOB | NULLABLE\\n- leadWebsite (None): ALPHANUMERIC | NULLABLE\\n- leadSubscriptions (None): REFERENCE | NULLABLE\\n- leadSubject (None): ALPHANUMERIC | NULLABLE\\n- leadDetails (None): MEMO | NULLABLE\\n- leadRqstInChannel (None): REFERENCE | NULLABLE\\n- leadContext (None): REFERENCE | NULLABLE\\n- leadRqstQualif (None): REFERENCE | NULLABLE\\n- leadManager (None): ALPHANUMERIC | NULLABLE\\n- leadOriginKey (None): FOB | NULLABLE\\n- leadSimpleStatus (None): REFERENCE | NULLABLE\\n- leadArchived (None): BOOLEAN | NULLABLE\\n- leadDataPrivacyConsentAlert (None): REFERENCE | NULLABLE\\n- leadJobTitle (None): ALPHANUMERIC | NULLABLE\\n- leadHeadCountCode (None): REFERENCE | NULLABLE\\n- leadActivity (None): REFERENCE | NULLABLE\\n- leadTransformCreateOpportunity (None): BOOLEAN | NULLABLE\\n- leadMemo2 (None): BLOB | NULLABLE\\n- leadText1 (None): ALPHANUMERIC | NULLABLE\\n- leadText2 (None): ALPHANUMERIC | NULLABLE\\n- leadText3 (None): ALPHANUMERIC | NULLABLE\\n- leadUtmSource (None): ALPHANUMERIC | NULLABLE\\n- leadUtmMedium (None): ALPHANUMERIC | NULLABLE\\n- leadUtmCampaign (None): ALPHANUMERIC | NULLABLE\\n- leadUtmTerm (None): ALPHANUMERIC | NULLABLE\\n- leadUtmContent (None): ALPHANUMERIC | NULLABLE\\n- leadAuthority (None): REFERENCE | NULLABLE\\n- leadNeed (None): MEMO | NULLABLE\\n- leadTiming (None): REFERENCE | NULLABLE\\n\\n# VALIDATION RULES\\n- All REQUIRED fields must be provided.\\n- Respect maximum sizes for VARCHAR-like fields.\\n- Ensure data types (numeric/date/text) align with field expectations.\\n\\n# FOREIGN KEY RELATIONSHIPS\\n(No foreign keys detected)\\n\\n# COMMON DATA MAPPING EXAMPLES\\nUser source column 'Full Name' -> map to field containing keyword: name\\nUser source column 'EmailAddress' -> map to field containing: email\\nUser source column 'Phone' -> map to field containing: phone\\nIf ambiguity (e.g., multiple phone fields), flag for human review instead of guessing.\\n\\n# INSERTION ORDER GUIDANCE\\nThis entity can be inserted independently (no FK dependencies detected).\\n\\n# RISK / AMBIGUITY FLAGS\\n- No required fields detected: verify if primary key is auto-generated.\\n\", 'chunk_type': 'table_ingestion_profile', 'primary_table': 'Lead', 'table_code': 'lead', 'table_kind': 'Entity', 'field_count': 60, 'metadata': {'chunk_type': 'table_ingestion_profile', 'primary_table': 'Lead', 'table_code': 'lead', 'table_kind': 'Entity', 'field_count': 60}}, vector=None, shard_key=None, order_value=None)]\n"
     ]
    }
   ],
   "source": [
    "query = \"Which table should I use to store email messages with sender recipient subject and body content\"\n",
    "query_embedding = embedding.embed_query(query)\n",
    "\n",
    "response = qdrant_client.query_points(\n",
    "    collection_name=\"sandbox_vector_store\",\n",
    "    query=query_embedding,\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a81219",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
